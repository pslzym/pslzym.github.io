<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[golang学习]]></title>
    <url>%2F2018%2F04%2F27%2Fgolang%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[数组、切片、映射数组内部实现 在Go语言里，数组是一个长度固定的数据类型，用于存储一段具有相同的类型的元素的连续块。数组存储的类型可以是内置类型也可以是某种结构类型。 声明和初始化声明数组时，需要指定内部的数据类型，以及需要存储的元素的数量。12341) var array [5]int2) array := [5]int&#123;10,20,30,40,50&#125;3) array := [...]int&#123;10,20,30,40,50&#125; //自动计算长度。4）array := [5]int&#123;1:10,2:50&#125; 数组相互赋值，在函数间传递数组数组相互赋值1234var array1 [5]stringarray2 := [5]string&#123;&quot;RED&quot;, &quot;Blue&quot;, &quot;Green&quot;&#125;array1 = array2两个赋完值之后，数组里面的值完全一样。完全拷贝了一份出来。 在函数间传递数组1根据内存和性能来看，在函数间传递数组是一个开销很大的操作。在函数间传递变量时，总是以值的方式传递。 切片切片的内部实现切片是一个很小的对象，对底层数据进行了抽象，并提供相关的操作函数。切片有三个字段的数据结构，分别为指向低层数组的指针、切片访问的低层元素的个数、切片准许增长到的元素的个数。 创建和初始化1.make 和切片字面量123slice := make([]string, 5) //长度和容量都等于5slice1 := make([]string, 5, 6) //长度等于5，容量等于6slice2 := []int&#123;10, 20, 30&#125; //长度和容量都等于3 2.nil和空切片123456789nil切片var slice []int空切片slice := make([]int,0)slice := []int&#123;&#125;切片只能和nil比较nil切片和nil值相等空切片和nil值不等 使用切片1.赋值12345678slice := []int&#123;10, 20, 30, 40， 50&#125;newSlice := slice[1:3]两个切片的低层数据是共用的。此时newSlice的长度为2，容量为4。当newSlice的容量没有超过4时，如果修改一个会影响另外一个的值。当newSlice的容量超过4时，会发生一次低层数据的拷贝。此后的修改两个切片的数据会互不干扰。newSlice := slice[1:3:3]此时的效果和上面一样，只不过容量会变为2。超过容量之后也会发生一次拷贝。之后的行为和上述一样。 映射内部的实现映射是一种数据结构，用于存储一系列无序的键值对。映射里面基于键来存储值。 创建和初始化创建12dict := make(map[string]int)dict := map[string]string&#123;&quot;RED&quot; : &quot;#daxx&quot;, &quot;Orange&quot; : &quot;#adxx&quot;&#125; 使用映射 为映射赋值 12colors := map[string]string&#123;&#125;colors[&quot;Red&quot;] = &quot;#daxxx&quot; 对nil映射赋值时的语言运行时错误 12var colors map[string]stringcolors[&quot;Red&quot;] = &quot;#daxxx&quot; //报错 从映射中取值 1234567891011121314a. value, exists := colors[&quot;Blue&quot;] if exists &#123; fmt.Println(value) &#125;b. value := colors[&quot;Blue&quot;] if value != &quot;&quot; &#123; fmt.Println(value) &#125; 即便是这个键不存在也总会返回一个值，在这种情况下，返回的是改值对应的类型的零值。 for value, key := range colors &#123; &#125; 删除映射里面的值 1delete(colors, &quot;psl&quot;) //删除键为psl的键值对。 在函数间传递 在函数间传递映射并不会制造出该映射的一个副本。实际上，当传递映射给一个函数，并对其作出修改时，所有对这个函数引用都会察觉到这个修改。 两个映射相互赋值也会和上面同样的效果 12345a := map[string]int&#123;&quot;1&quot;: 1, &quot;2&quot;: 2&#125;b := ab[&quot;1&quot;] = 2fmt.Println(a)fmt.Println(b) 打印出来一样 Go语言类型系统父类型不能接受子类型的值。 用户定义的类型123456789type user struct &#123; name string, email string, ext int, privileged bool&#125;var bill user当声明变量时，这个变量对应的值总是被初始化。这个值要么用指定的值初始化，要么用零值做初始化。 方法方法能给用户定义的类型添加新的行为。方法实际也是函数，只是在关键字func和方法名之间增加了一个参数。 Go语言里有两种类型的接收者：值接收者和指针接收者。 1234561.使用值作为接收者进行调用，方法会接收到值的一个副本。2.使用指针变量来调用方法，方法会共享调用方法时接收者所指向的值。Go语言在调用方法时，会调整接收者。使其满足函数的调用。bill.changeEmail(). -&gt; (&amp;bill).changeEmail()bill.changeEmail(). -&gt; (*bill).changeEmail() 内置类型，引用类型Go语言里的引用类型有如下几个：切片，映射，通道，接口，函数类型。 接口多态是指代码可以根据类型的具体实现采取不同行为的能力。 实现接口就是用来定义行为的类型。这些被定义的行为不由接口直接实现，而是通过方法由用户定义的类型实现。如果用户定义的类型实现了某个接口类型声明的一组方法，那么这个用户定义的类型的值就可以赋给这个接口类型的值。这个赋值会把用户定义的类型的值存入接口类型的值。接口的值是一个两个字长度的数据结构，第一个字包含一个指向内部表的指针。这个内部的表叫做iTable,包含了所存储的值的类型信息（值的类型信息和这个值关联的一组方法）。第二个字是一个指向所存储值的指针。将类型信息和指针组合在一起，就将这两个值组成了一种特殊的关系。 重点：方法集定义了一组关联到给定类型的值或者指针的方法。定义方法时使用的接收者的类型决定了这个方法是关联到值，还是关联到指针，还是两者都关联。从接收者的角度来看方法集 Methods Receivers Values (t T) T and *T (t *T) *T 并发与并行并行的关键是同时做很多事情。并发是指同时管理很多事情，这些事情可能只做了一半就被暂停去做别的事情了。 goroutine 如果希望让goroutine并行，必须使用多余一个的逻辑处理器。当有多个逻辑处理器时，调度器会将goroutine平等分配到每个逻辑处理器上。这会让goroutine在不同的线程上运行。不过要想实现并行的效果，用户需要让自己的程序运行在有多个物理处理器的机器上。 12runtime.GOMAXPROCS(1)//分配一个逻辑处理器给调度器使用runtime.GOMAXPROCS(runtime.NumCPU()) //给每个可用的核心分配一个逻辑处理器 竞争状态如果两个或者多个goruntine在没有互相同步的情况下，访问某个共享的资源，并试图同时读和写这个资源，就处于相互竞争的状态，这种情况被称作竞争状态。 锁住共享资源1234561.原子操作 atomic2.互斥锁 mutex runtine.Gosched()// 当前goruntine从线程退出，并放回到队列。 通道1.创建12unbufferd := make(chan int)bufferd := make(chan int, 10)]]></content>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书之约]]></title>
    <url>%2F2018%2F04%2F21%2F%E8%AF%BB%E4%B9%A6%E4%B9%8B%E7%BA%A6%2F</url>
    <content type="text"><![CDATA[大概从我记事开始，我自己就觉得自己不爱读书。小学的时候最怕的就是语文考试，每次语文考试的作文都是最令我头痛的。这种讨厌的状态一直持续到我高中毕业。 刚刚上大学那会，觉得什么都很新奇。我上的大学虽然不是特别好，但是有一个新建图书馆，特别大。由于图书馆特别大，并且藏书非常丰富，每次走进读书馆内心都充满略带严肃和安详的心情，那个时候才产生想要好好读几本书的想法。 不得不说开始读书的前几本书对一个人很重要。一旦你认识到读书的乐趣，那便一发不可收拾。还记得那个时候对我触动很大的一本《明朝那些事》。作者用非常有趣的写作手法，生动的描写了那个时代的人和事，我觉得很有趣，收获了很多生活的态度和哲学。自从那个时候开始，我的读书之路便像乘坐上了飞机，一发不可收拾。那个时候我的时间除了上课和在实验室做实验其他的时间基本全部用来读书。 刚刚开始读明朝史–&gt;美国历史和政治–&gt;罗马帝国历史–&gt;欧洲中世纪–&gt;欧洲现代史–&gt;全球历史–&gt;各国文学作品–&gt;入门的哲学–&gt;入门的经济学–&gt;什么都读。现在的我，基本上什么类型的书都读，觉得目前没有意思的书，放一段时间之后再读，可能就觉得有趣了。 我读书基本没有什么太大的功利目的。记得有一位名人说过，大概意思是：“我读书就是为了多了解些有趣的事情。”，我大概也是这个目的，不想让自己本以平凡的生命过的太过乏味和庸俗。我记忆力不是很好，基本上读过的书很快就会忘记，作者的名字我目前记住的不超过十位。虽然，我容易忘记作者名和书中大部分内容，但是我还是愿意继续的读下去，因为读书的乐趣是目前我所接触到的活动中给我持续快乐最多的一项。 今天写这篇文章，有几个目的。第一个目的就是鼓励自己要在自己的个人博客上面多写文章。记住要多写多写，熟能生巧。第二个目的就是要求自己以后每读一本书都要写一篇感悟，或者记录自己读这本书中有意义的东西。 要多写文章（技术的和生活的都要写） 看完一本书之后一定要写感悟或者留下一点记录 这两点以后一定要执行，在这里记录下，监督自己。 2018-4-21杭州家中]]></content>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 内存管理]]></title>
    <url>%2F2016%2F11%2F30%2Flinux-nei-cun-guan-li%2F</url>
    <content type="text"><![CDATA[内存管理内存管理逻辑概念 逻辑地址：是在机器语言指令中,来说明操作数和指令的地址;每个逻辑地址包括两部分:段(Segment)和偏移量(Offset)。 线性地址：也通常成为虚拟地址,在32位系统中,它是32位的无符号整型,最大可以达到4G。 物理地址：就是真正物理内存上的地址。 三种地址转换图 分段和分页机制 逻辑地址到线性地址图 线性地址到物理地址 123456会把线性地址分为：10：10：12每一个线程的PageDirectory的初始地址都不同，保存在CR3寄存器里面。CR3所指定的就是就是PageDirectory的结构地址。PageDirectory占4k，一个页。每一项占4个字节。每一个PageTable表项也占4k，一个页。每一项占4个字节。以上两个数据结构每个进程是独立的。都分配在3G~4G的内核内存空间。 内存管理相关算法 buddy : 解决空闲页的问题 12在实际应用中，经常需要分配一组连续的页框，而频繁地申请和释放不同大小的连续页框，必然导致在已分配页框的内存块中分散了许多小块的 空闲页框。这样，即使这些页框是空闲的，其他需要分配连续页框的应用也很难得到满足。为了避免出现这种情况，Linux内核中引入了伙伴系统算法(buddy system)。把所有的空闲页框分组为11个 块链表，每个块链表分别包含大小为1，2，4，8，16，32，64，128，256，512和1024个连续页框的页框块。最大可以申请1024个连 续页框，对应4MB大小的连续内存。每个页框块的第一个页框的物理地址是该块大小的整数倍。 slab : 用于解决特定大小缓存浪费块空间的问题。 12工作于物理内存页框分配器之上，管理特定大小对象的缓存，进行快速而高效的内存分配。slab分配器为每种使用的内核对象建立单独的缓冲区。Linux 内核已经采用了伙伴系统管理物理内存页框，因此 slab分配器直接工作于伙伴系 统之上。每种缓冲区由多个 slab 组成，每个 slab就是一组连续的物理内存页框，被划分成了固定数目的对象。根据对象大小的不同，缺省情况下一个 slab 最多可以由 1024个页框构成。出于对齐 等其它方面的要求，slab 中分配给对象的内存可能大于用户要求的对象实际大小，这会造成一定的 内存浪费。 内存分配相关函数 __get_free_pages 12 unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order) __get_free_pages函数是最原始的内存分配方式，直接从伙伴系统中获取原始页框，返回值为第一个页框的起始地址 kmem_cache_create/ kmem_cache_alloc是基于slab分配器的一种内存分配方式，适用于反复分配释放同一大小内存块的场合。首先用kmem_cache_create创建一个高速缓存区域，然后用kmem_cache_alloc从 该高速缓存区域中获取新的内存块。 kmalloc 12 void *kmalloc(size_t size, gfp_t flags) kmalloc是内核中最常用的一种内存分配方式，它通过调用kmem_cache_alloc函 数来实现。kmalloc一次最多能申请的内存大小由include/linux/Kmalloc_size.h的 内容来决定. vmalloc 123 void *vmalloc(unsigned long size) 前面几种内存分配方式都是物理连续的，能保证较低的平均访问时间。但是在某些场合中，对内存区的请求不是很频繁，较高的内存访问时间也 可以接受，这是就可以分配一段线性连续，物理不连续的地址，带来的好处是一次可以分配较大块的内存。vmalloc对 一次能分配的内存大小没有明确限制。出于性能考虑，应谨慎使用vmalloc函数。在测试过程中， 最大能一次分配1GB的空间。]]></content>
      <tags>
        <tag>
- Linux操作运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop HA 部署方案]]></title>
    <url>%2F2016%2F10%2F09%2Fhadoop-ha-bu-shu-fang-an%2F</url>
    <content type="text"><![CDATA[##Hadoop HA 部署方案 原理 部署方法 ###原理 原理图 1234567891011HA的方案比单节点Master多了几个组件：zookeeper集群： 主要是用来维护和选举MasterZKFC: 每一个master上必须运行一个，主要是用来监控master的状态，为选举active的master做状态监控。JN（JournalNode）集群： 主要是用来存储Master的原始数据，log和Matadata。防止log和matadata丢失。``` ###部署方法官方连接：https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html#Configuration_details主要配置文件:hdfs-site.xml dfs.nameservices pslcluster dfs.ha.namenodes.pslcluster nn1,nn2 dfs.namenode.rpc-address.pslcluster.nn1 master:8020 dfs.namenode.rpc-address.pslcluster.nn2 slave4:8020 dfs.namenode.http-address.pslcluster.nn1 master:50070 dfs.namenode.http-address.pslcluster.nn2 slave4:50070 dfs.namenode.shared.edits.dir qjournal://slave1:8485;slave2:8485;slave3:8485/mycluster dfs.client.failover.proxy.provider.pslcluster org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider dfs.ha.fencing.methods sshfence dfs.ha.fencing.ssh.private-key-files /home/hadoop/.ssh/id_rsa &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoopHA/journal/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 12core-site.xml fs.defaultFS hdfs://pslcluster ha.zookeeper.quorum master:2181,slave1:2181,slave2:2181 hadoop.tmp.dir /usr/local/hadoopHA/tmp 12slaves Slave1Slave2Slave3``` 还必须配置一下hadoop-env.sh 中的java环境 启动过程： 首先手动启动journalnodehadoop-daemon.sh start journalnode在相关的 在其中一台namenode上面格式化namenode hdfs namenode -format 启动刚刚格式化完成的namenode :启动一个namenode 在另外一台master上面的copy相关原始数据hdfs namenode -bootstrapStandby 停止所有服务./stop-all.sh 初始化zkfchdfs zkfc -formatZK 启动集群start-dfs.sh 部署好之后可以看一下相关的进程和端口：http://master:50070尝试put一个数据到相关的集群。]]></content>
      <tags>
        <tag>
- Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Trident 理论学习]]></title>
    <url>%2F2016%2F10%2F08%2Ftrident-li-lun-xue-xi%2F</url>
    <content type="text"><![CDATA[##Trident 理论学习 ###Trident API – SpoutITridentSpout :最通用的Spout,可以支持事务或者不透明事务语义。IBatchSpout : 一个非事务spout。IPartitionedTridentSpout: 分区事务spout，从数据源（eg:Kafka集群）读分区数据。IOpaquePartitionedTridentSpout: 不透明分区事务spout，从数据源读分区数据。 ###Trident API – Bolt唯一显性Bolt接口：ITridentBatchBolt，但很少用。Trident编程特点就是Stream。Trident的topology会被编译成尽可能高效的Storm topology。只有在需要对数据进行repartition的时候（如groupby或者shuffle）才会把tuple通过network发送出去. ###Trident 概念之OperationOperation 类相关概念： Function ， 如BaseFunction, 如TridentWordCount 中用的split、如BaseQueryFunction, TridentWordCount中用的MapGet,从State中查询 Filter : 如BaseFilter， 如TridentWordCount 中用的FilterNull 聚合类： CombinerAggregator&lt;T&gt; Aggregator&lt;T&gt; ReducerAggregator&lt;T&gt; Function: 1234567 public class MyFunction extends BaseFunction &#123; public void execute(TridentTuple tuple, TridentCollector collector) &#123; for(int i=0; i &lt; tuple.getInteger(0); i++) &#123; collector.emit(new Values(i)); &#125; &#125;&#125; 假设有一个叫“mystream”输入流有【“a”，“b”，“c”】三个字段【1，2，3】【4，1，6】【3，0，8】 运行mystream.each(new Fields(“b”), new MyFunction(), new Fields(“d”)) 运行的结果将会有四个字段【“a”, “b”, “c”, “d”】 [1,2,3,0] [1,2,3,1] [4,1,6,0] Filters: Filters接收一个元组（tuple），决定是否需要继续保留这个元组。比如： 12345public class MyFilter extends BaseFilter&#123; public booleanisKeep(TridentTuple tuple) &#123; return tuple.getInteger(0) == 1 &amp;&amp; tuple.getInteger(1) == 2; &#125;&#125; 假如有如下输入：【1，2，3】【2，1，1】【2，3，4】运行下面的代码：mystream.each(new Fields(“b”, “a”), new MyFilter())结果将会是：【2，1，1】 聚合：例子：persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields(“count”)) 在持久化中使用到。new Count就是具体的聚合类。 聚合接口：CombinerAggreator：在每个tuple上运行init，使用combine去联合结果 。如果批次没有数据，运行zero函数。ReduceAggregator:在init()初始化的时候产生一个值，每个输入的元组在这个值的基础上进行迭代并输出一个单独的值Aggregate：功能最强大 Init函数在执行批次操作之前被调用，并返回一个state对象，这个对象将会会传入到aggregate和complete函数中。 Aggregate会对批次中每个tuple调用，这个方法可以跟新state也可以发射（emit）tuple。 当这个批次分区的数据执行结束后调用complete函数。 aggregate和persistentAggregate函数对流做聚合。Aggregate在每个批次上独立运行，persistentAggregate聚合流的所有的批次并将结果存储下来。 12345678910111213141516public class Sum implements CombinerAggregator&lt;Number&gt; &#123; public Sum() &#123; &#125; public Number init(TridentTuple tuple) &#123; return (Number)tuple.getValue(0); &#125; public Number combine(Number val1, Number val2) &#123; return Numbers.add(val1, val2); &#125; public Number zero() &#123; return Integer.valueOf(0); &#125; &#125;]]></content>
      <tags>
        <tag>
- storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm不透明事务示例]]></title>
    <url>%2F2016%2F10%2F08%2Fstormbu-tou-ming-shi-wu-shi-li%2F</url>
    <content type="text"><![CDATA[##Storm不透明事务示例在IPartitionedTransactionalSpout 案例基础上用IOpaquePartitionedTransactionalSpout 实现实现分析：需调整Spout端 和 Bolt端Bolt端主要是调整写库部分 class DbValue {String dateStr; // 按日期汇总int count = 0; // 汇总数BigInteger txid; // 事务IDInt prev_count ; // 上个事务批次处理后的结果}DbValue模拟一个数据表 对于commiter bolt必须要记录上一次事务tuple的个数，因为可能同一个事务重发之后 MyOpaquePtTxSpout.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class MyOpaquePtTxSpout implements IOpaquePartitionedTransactionalSpout &#123; public static int BATCH_NUM = 10 ; public Map&lt;Integer, Map&lt;Long,String&gt;&gt; PT_DATA_MP = new HashMap&lt;Integer, Map&lt;Long,String&gt;&gt;(); public MyOpaquePtTxSpout() &#123; Random random = new Random(); String[] hosts = &#123; &quot;www.taobao.com&quot; &#125;; String[] session_id = &#123; &quot;ABYH6Y4V4SCVXTG6DPB4VH9U123&quot;, &quot;XXYH6YCGFJYERTT834R52FDXV9U34&quot;, &quot;BBYH61456FGHHJ7JL89RG5VV9UYU7&quot;, &quot;CYYH6Y2345GHI899OFG4V9U567&quot;, &quot;VVVYH6Y4V4SFXZ56JIPDPB4V678&quot; &#125;; String[] time = &#123; &quot;2014-01-07 08:40:50&quot;, &quot;2014-01-07 08:40:51&quot;, &quot;2014-01-07 08:40:52&quot;, &quot;2014-01-07 08:40:53&quot;, &quot;2014-01-07 09:40:49&quot;, &quot;2014-01-07 10:40:49&quot;, &quot;2014-01-07 11:40:49&quot;, &quot;2014-01-07 12:40:49&quot; &#125;; for (int j = 0; j &lt; 5; j++) &#123; HashMap&lt;Long, String&gt; dbMap = new HashMap&lt;Long, String&gt; (); for (long i = 0; i &lt; 100; i++) &#123; dbMap.put(i,hosts[0]+&quot;\t&quot;+session_id[random.nextInt(5)]+&quot;\t&quot;+time[random.nextInt(8)]); &#125; PT_DATA_MP.put(j, dbMap); &#125; &#125; public Emitter getEmitter(Map map, TopologyContext topologyContext) &#123; return new MyEmiiter(); &#125; public Coordinator getCoordinator(Map map, TopologyContext topologyContext) &#123; return new MyCoordinator(); &#125; public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; &#125; public Map&lt;String, Object&gt; getComponentConfiguration() &#123; return null; &#125; public class MyCoordinator implements IOpaquePartitionedTransactionalSpout.Coordinator&#123; public boolean isReady() &#123; Utils.sleep(2000); return true; &#125; public void close() &#123; &#125; &#125; public class MyEmiiter implements IOpaquePartitionedTransactionalSpout.Emitter&lt;MyMata&gt;&#123; public MyMata emitPartitionBatch(TransactionAttempt transactionAttempt, BatchOutputCollector batchOutputCollector, int partition, MyMata myMata) &#123; // TODO Auto-generated method stub System.err.println(&quot;emitPartitionBatch partition:&quot;+partition); long beginPoint = 0; if (myMata == null) &#123; beginPoint = 0 ; &#125;else &#123; beginPoint = myMata.getBeginPoint() + myMata.getNum() ; &#125; MyMata mata = new MyMata() ; mata.setBeginPoint(beginPoint); mata.setNum(BATCH_NUM); System.err.println(&quot;启动一个事务：&quot;+mata.toString()); Map&lt;Long, String&gt; batchMap = PT_DATA_MP.get(partition); for (Long i = mata.getBeginPoint(); i &lt; mata.getBeginPoint()+mata.getNum(); i++) &#123; if (batchMap.size()&lt;=i) &#123; break; &#125; batchOutputCollector.emit(new Values(transactionAttempt, batchMap.get(i))); &#125; return mata; &#125; public int numPartitions() &#123; return 5; &#125; public void close() &#123; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>
- storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm分区事务示例程序]]></title>
    <url>%2F2016%2F10%2F08%2Fstormfen-qu-shi-wu-shi-li-cheng-xu%2F</url>
    <content type="text"><![CDATA[##Storm分区事务示例程序 分区事务和事务相关代码的区别主要在于spout端的代码 MyPtTxSpout.java //分区事务的spout端代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public Map&lt;Integer, Map&lt;Long,String&gt;&gt; PT_DATA_MP = new HashMap&lt;Integer, Map&lt;Long,String&gt;&gt;(); public static int BATCH_NUM = 10 ; public MyPtTxSpout() &#123; Random random = new Random(); String[] hosts = &#123; &quot;www.taobao.com&quot; &#125;; String[] session_id = &#123; &quot;ABYH6Y4V4SCVXTG6DPB4VH9U123&quot;, &quot;XXYH6YCGFJYERTT834R52FDXV9U34&quot;, &quot;BBYH61456FGHHJ7JL89RG5VV9UYU7&quot;, &quot;CYYH6Y2345GHI899OFG4V9U567&quot;, &quot;VVVYH6Y4V4SFXZ56JIPDPB4V678&quot; &#125;; String[] time = &#123; &quot;2014-01-07 08:40:50&quot;, &quot;2014-01-07 08:40:51&quot;, &quot;2014-01-07 08:40:52&quot;, &quot;2014-01-07 08:40:53&quot;, &quot;2014-01-07 09:40:49&quot;, &quot;2014-01-07 10:40:49&quot;, &quot;2014-01-07 11:40:49&quot;, &quot;2014-01-07 12:40:49&quot; &#125;; for (int j = 0; j &lt; 5; j++) &#123; HashMap&lt;Long, String&gt; dbMap = new HashMap&lt;Long, String&gt; (); for (long i = 0; i &lt; 100; i++) &#123; dbMap.put(i,hosts[0]+&quot;\t&quot;+session_id[random.nextInt(5)]+&quot;\t&quot;+time[random.nextInt(8)]); &#125; PT_DATA_MP.put(j, dbMap); &#125; &#125; public Coordinator getCoordinator(Map map, TopologyContext topologyContext) &#123; return new MyCoordinator(); &#125; public Emitter&lt;MyMata&gt; getEmitter(Map map, TopologyContext topologyContext) &#123; return new MyEmitter(); &#125; public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(&quot;tx&quot;, &quot;log&quot;)); &#125; public Map&lt;String, Object&gt; getComponentConfiguration() &#123; return null; &#125; public class MyCoordinator implements IPartitionedTransactionalSpout.Coordinator&#123; public int numPartitions() &#123; ///返回有几个分区 return 5; &#125; public boolean isReady() &#123; Utils.sleep(2000); return true; &#125; public void close() &#123; &#125; &#125; public class MyEmitter implements IPartitionedTransactionalSpout.Emitter&lt;MyMata&gt;&#123; public MyMata emitPartitionBatchNew(TransactionAttempt transactionAttempt, BatchOutputCollector batchOutputCollector, int partition, MyMata myMata) &#123; ///新的事务调用的发送函数 //此函数可以和非分区的事务代码中 Coordinator做对比 基本差不多 long beginPoint = 0; if(myMata == null)&#123; beginPoint = 0; &#125;else&#123; beginPoint = myMata.getBeginPoint() + myMata.getNum(); &#125; MyMata myMata1 = new MyMata(); myMata1.setNum(BATCH_NUM); myMata1.setBeginPoint(beginPoint); //启动一个事务 emitPartitionBatch(transactionAttempt, batchOutputCollector, partition, myMata1); return myMata1; &#125; public void emitPartitionBatch(TransactionAttempt transactionAttempt, BatchOutputCollector batchOutputCollector, int partition, MyMata myMata) &#123; //失败的事务调用的函数 long beginPoint = myMata.getBeginPoint() ; int num = myMata.getNum() ; Map&lt;Long, String&gt; batchMap = PT_DATA_MP.get(partition); for (long i = beginPoint; i &lt; num+beginPoint; i++) &#123; if (batchMap.get(i)==null) &#123; break; &#125; batchOutputCollector.emit(new Values(transactionAttempt,batchMap.get(i))); &#125; &#125; public void close() &#123; &#125; &#125;]]></content>
      <tags>
        <tag>
- storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm事务示例程序分析]]></title>
    <url>%2F2016%2F10%2F08%2Fstormshi-wu-shi-li-cheng-xu-fen-xi%2F</url>
    <content type="text"><![CDATA[##Storm事务示例程序分析 MyMata.java 原始数据类 123456789101112131415161718192021222324252627public class MyMata implements Serializable &#123; private long beginPoint; // 事务开始位置 private int num; private static final long serialVersionUID = 1L; @Override public String toString() &#123; return &quot;beginPoint : &quot; + getBeginPoint() + &quot;---- len : &quot; + getNum(); &#125; public void setBeginPoint(long beginPoint) &#123; this.beginPoint = beginPoint; &#125; public void setNum(int num) &#123; this.num = num; &#125; public long getBeginPoint() &#123; return beginPoint; &#125; public int getNum() &#123; return num; &#125;&#125; MyTxSpout.java spout类，产生原始数据。 12345678910111213141516171819202122232425262728293031323334353637public class MyTxSpout implements ITransactionalSpout&lt;MyMata&gt; &#123; Map&lt;Long, String&gt; dbMap = null; //private static final long serialVersionUID = 1L; public MyTxSpout()&#123; Random random = new Random(); dbMap = new HashMap&lt;Long, String&gt;(); String[] hosts = &#123; &quot;www.taobao.com&quot; &#125;; String[] session_id = &#123; &quot;ABYH6Y4V4SCVXTG6DPB4VH9U123&quot;, &quot;XXYH6YCGFJYERTT834R52FDXV9U34&quot;, &quot;BBYH61456FGHHJ7JL89RG5VV9UYU7&quot;, &quot;CYYH6Y2345GHI899OFG4V9U567&quot;, &quot;VVVYH6Y4V4SFXZ56JIPDPB4V678&quot; &#125;; String[] time = &#123; &quot;2014-01-07 08:40:50&quot;, &quot;2014-01-07 08:40:51&quot;, &quot;2014-01-07 08:40:52&quot;, &quot;2014-01-07 08:40:53&quot;, &quot;2014-01-07 09:40:49&quot;, &quot;2014-01-07 10:40:49&quot;, &quot;2014-01-07 11:40:49&quot;, &quot;2014-01-07 12:40:49&quot; &#125;; for (long i = 0; i &lt; 100; i++) dbMap.put(i, hosts[0] + &quot;\t&quot; + session_id[random.nextInt(5)] + &quot;\t&quot; + time[random.nextInt(8)]); &#125; public Coordinator&lt;MyMata&gt; getCoordinator(Map map, TopologyContext topologyContext) &#123; return new MyCoordinator(); ///用于发送和保存事务相关信息 &#125; public Emitter&lt;MyMata&gt; getEmitter(Map map, TopologyContext topologyContext) &#123; return new MyEmitter(dbMap);//用于发送每一个事务具体的数据 &#125; public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(&quot;tx&quot;, &quot;log&quot;)); &#125; public Map&lt;String, Object&gt; getComponentConfiguration() &#123; return null; &#125;&#125; MyCoordinator.java //用于为每一次事务设定原始数据，并启动事务 123456789101112131415161718192021222324252627282930public class MyCoordinator implements ITransactionalSpout.Coordinator&lt;MyMata&gt; &#123; private static int BATCH_NUM = 10; public MyMata initializeTransaction(BigInteger bigInteger, MyMata preMyMata) &#123; long beginPoint = 0; if (preMyMata == null)&#123; beginPoint = 0; &#125;else &#123; beginPoint = preMyMata.getBeginPoint() + preMyMata.getNum(); &#125; MyMata nowMyMata = new MyMata(); nowMyMata.setBeginPoint(beginPoint); nowMyMata.setNum(BATCH_NUM); System.err.println(&quot;MyCoordinator initializeTransaction 启动一个事务: &quot; + nowMyMata.toString()); return nowMyMata; &#125; public boolean isReady() &#123; Utils.sleep(2000); return true; &#125; public void close() &#123; &#125;&#125; MyEmitter.java //获取Coordinator里面设定的原始数据，并根据相关类容发送相关tuple 12345678910111213141516171819202122232425262728public class MyEmitter implements ITransactionalSpout.Emitter&lt;MyMata&gt; &#123; Map&lt;Long, String&gt; dbMap = null; public MyEmitter(Map&lt;Long, String&gt; dbMap)&#123; this.dbMap = dbMap; &#125; public void emitBatch(TransactionAttempt transactionAttempt, MyMata myMata, BatchOutputCollector batchOutputCollector) &#123; long beginPoint = myMata.getBeginPoint(); int num = myMata.getNum(); for(long i = beginPoint; i &lt; beginPoint + num; i++)&#123; if (dbMap.get(i) == null)&#123; continue; &#125; batchOutputCollector.emit(new Values(transactionAttempt, dbMap.get(i))); &#125; &#125; public void cleanupBefore(BigInteger bigInteger) &#123; &#125; public void close() &#123; &#125;&#125; MyTransactionalBolt.java //一级bolt。写代码的时候注意，每一次batch事务过来后，都会重新生成新德MyTransactionalBolt类。类里面相关的临时数据会全部消失。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class MyTransactionalBolt extends BaseTransactionalBolt &#123; private TransactionAttempt tx = null; private BatchOutputCollector batchOutputCollector = null; //private static final long serialVersionUID = 1L; int count = 0;///一次批处理事务会调用一次 public void prepare(Map map, TopologyContext topologyContext, BatchOutputCollector batchOutputCollector, TransactionAttempt transactionAttempt) &#123; this.tx = transactionAttempt; this.batchOutputCollector = batchOutputCollector; System.err.println(&quot;MyTransactionalBolt prepare TransactionId: &quot; + transactionAttempt.getTransactionId() + &quot;---- AttemptId:&quot; + transactionAttempt.getAttemptId()); &#125;////对每一个tuple都会调用一次 public void execute(Tuple tuple) &#123; TransactionAttempt txExe = (TransactionAttempt)tuple.getValueByField(&quot;tx&quot;); System.err.println(&quot;MyTransactionalBolt execute TransactionId: &quot; + txExe.getTransactionId() + &quot;---- AttemptId:&quot; + txExe.getAttemptId()); String log = tuple.getStringByField(&quot;log&quot;); if (log != null &amp;&amp; log.length() &gt; 0)&#123; count++; &#125; &#125;////一次批处理事务会调用一次 public void finishBatch() &#123; System.err.println(&quot;MyTransactionalBolt finishBatch : &quot; + count); this.batchOutputCollector.emit(new Values(tx, count)); &#125; public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(&quot;tx&quot;, &quot;count&quot;)); &#125;&#125; MyCommitter.java //批处理会按照事务的顺序进行committer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class MyCommitter extends BaseTransactionalBolt implements ICommitter &#123; //private static final long serialVersionUID = 1L; int sum = 0; TransactionAttempt tx = null; BatchOutputCollector batchOutputCollector = null; public static final String GLOBAL_KEY = &quot;GLOBAL_KEY&quot;; ///这里是静态的，不管来多少次事务，里面的相关数据都会保存。 public static Map&lt;String, DbValue&gt; dbMap = new HashMap&lt;String, DbValue&gt;() ; //public Map&lt;String, DbValue&gt; dbMap = new HashMap&lt;String, DbValue&gt;() ; public void prepare(Map map, TopologyContext topologyContext, BatchOutputCollector batchOutputCollector, TransactionAttempt transactionAttempt) &#123; this.tx = transactionAttempt; this.batchOutputCollector = batchOutputCollector; System.err.println(&quot;MyCommitter prepare : &quot; + tx.getTransactionId() + &quot; : &quot; + tx.getAttemptId()); &#125; public void execute(Tuple tuple) &#123; sum += tuple.getInteger(1); System.err.println(&quot;MyCommitter execute : &quot; + tx.getTransactionId() + &quot; : &quot; + tx.getAttemptId()); &#125;////事务处理完成后，会按照顺序来进行提交 public void finishBatch() &#123; System.err.println(&quot;MyCommitter finishBatch : &quot; + tx.getTransactionId() + &quot; : &quot; + tx.getAttemptId()); DbValue value = dbMap.get(GLOBAL_KEY); DbValue newValue; if (value == null || !value.txid.equals(tx.getTransactionId()))&#123; newValue = new DbValue(); newValue.txid = tx.getTransactionId(); if (value == null)&#123; newValue.count = sum; &#125;else &#123; newValue.count = sum + value.count; &#125; dbMap.put(GLOBAL_KEY, newValue); &#125; System.out.println(&quot;total==========================:&quot;+dbMap.get(GLOBAL_KEY).count); &#125; public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; &#125; public static class DbValue&#123; BigInteger txid; int count = 0; &#125;&#125; MyTopo.java ///事务top的相关编写 123456789101112131415161718192021222324252627public class MyTopo &#123; public static void main(String [] args)&#123; TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder(&quot;ttbId&quot;,&quot;spoutid&quot;,new MyTxSpout(),1); builder.setBolt(&quot;bolt1&quot;, new MyTransactionalBolt(),3).shuffleGrouping(&quot;spoutid&quot;); builder.setBolt(&quot;committer&quot;, new MyCommitter(),1).shuffleGrouping(&quot;bolt1&quot;) ; Config conf = new Config() ; conf.setDebug(true); if (args.length &gt; 0) &#123; try &#123; StormSubmitter.submitTopology(args[0], conf, builder.buildTopology()); &#125; catch (AlreadyAliveException e) &#123; e.printStackTrace(); &#125; catch (InvalidTopologyException e) &#123; e.printStackTrace(); &#125; catch (AuthorizationException e) &#123; e.printStackTrace(); &#125; &#125;else &#123; LocalCluster localCluster = new LocalCluster(); localCluster.submitTopology(&quot;mytopology&quot;, conf, builder.buildTopology()); &#125; &#125;&#125;]]></content>
      <tags>
        <tag>
- storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm事务-批处理原理]]></title>
    <url>%2F2016%2F10%2F08%2Fstormshi-wu-pi-chu-li-yuan-li%2F</url>
    <content type="text"><![CDATA[##Storm事务-批处理原理 事务对于容错机制，Storm通过一个系统级别的组件acker，判断tuple是否发送成功，进而spout可以重发该tuple，保证一个tuple在出错的情况下至少被重发一次。但是在需要精确统计tuple的场景下（销售金额），希望每个tuple“被且仅被处理一次”。storm引入了Transactional Topology。它可以保证每一个“tuple”被且仅被处理一次，这样就可以实现一种非常准确，且高度容错方式来实现计数类应用。 批处理逐个处理单个tuple，增加很多开销，如写库、输出结果频率过高。事务处理单个tuple效率比较低，因此storm中引入batch处理。批处理是一次性处理一批（batch）tuple，事务可以确保该批次要么全部处理成功，如果有处理失败的则全部不计，Storm会对失败的批次重新发送，且确保每一个batch被且仅被处理一次。 ###事务机制和原理对于只处理一次的需要，从原理上来讲，需要在发送tuple的时候带上，txid，在需要事务处理的时候，根据该txid是否以前已经处理成功来决定是否进行处理，当然需要把txid和处理结果一起做保存。在事务batch处理中，一批tuple赋予一个txid，为了提高batch之间处理的并行度，storm采用了pipeline处理模型，这样多个事务可以并行执行，但是commit的是严格按照顺序的。 Storm事务处理中，把一个batch的计算分成两个阶段processing和commit阶段：processing阶段：多个batch可以并行计算。commit阶段：batch之间强制按照顺序进行提交。 ####事务Topologies使用Transactional Topologies的时候，storm为你做下面这些事情。 管理状态：Storm把所有实现Transactional Topologies所必须的状态保存在zookeeper里面，包括当前transaction id 以及定义每一个batch的一些元数据。 协调事务：Storm帮你管理所有事情，如帮你决定在任何一个时间点是该processing还是该committing. 错误检测: Storm利用acking框架来高效地检测什么时候一个batch被成功处理了，被成功提交了，或者失败了。Storm然后会相应地replay对应的batch。你不需要自己手 动做任何acking或者anchoring (emit时发生的动作)。 内置的批处理API: Storm在普通bolt之上包装了一层API来提供对tuple的批处理支持。Storm管理所有的协调工作，包括决定什么时候一个bolt接收到一个特定transaction的所有tuple。Storm同时也会自动清理每个transaction所产生的中间数据。 事务性的spout需要实现ITransactionalSpout，这个接口包含两个内部接口类Coordinator和Emitter。在topology运行的时候，事务性的spout内部包含一个子Topology，结构图如下： 这里面有两种类型的tuple，一种是事务性的tuple，一种是batch中的tuple；coordinator 开启一个事务准备发射一个batch时候，进入一个事务的processing阶段，会发射一个事务性 tuple(transactionAttempt &amp; metadata)到”batch emit”流 Emitter以all grouping(广播)的方式订阅coordinator的”batch emit”流，负责为每个batch实际发射tuple。发送的tuple都必须以TransactionAttempt作为第一个field，storm根据这个field来判断tuple属于哪一个batch。 coordinator只有一个，emitter根据并行度可以有多个实例 coordinator只有一个，emitter根据并行度可以有多个实例 ####TransactionAttempt 和 元数据 TransactionAttempt包含两个值：一个transaction id，一个attempt id。transaction id的作用就是我们上面介绍的对于每个batch中的tuple是唯一的，而且不管这个batch replay多少次都是一样的。attempt id是对于每个batch唯一的一个id， 但是对于同一个batch，它replay之后的attempt id跟replay之前就不一样了，我们可以把attempt id理解成replay-times， storm利用这个id来区别一个batch发射的tuple的不同版本 metadata(元数据)中包含当前事务可以从哪个point进行重放数据，存放在zookeeper中的，spout可以通过Kryo从zookeeper中序列化和反序列化该元数据。 内部处理流程 ####事务性Bolt BaseTransactionalBolt处理batch在一起的tuples，对于每一个tuple调用调用execute方 法，而在整个batch处理(processing)完成的时候调用finishBatch方法。如果BatchBolt被标记成Committer，则 只能在commit阶段调用finishBatch方法。一个batch的commit阶段由storm保证只在前一个batch成功提交之后才会执行。并且它会重试直到topology里面的所有bolt在commit完成提交。那么如何知道batch的processing完成了，也就是bolt是否接收处理了batch里面所有的tuple；在bolt内部，有一个 CoordinatedBolt的模型。CoordinateBolt具体原理如下： 每个CoordinateBolt记录两个值：有哪些task给我发送了tuple（根据topology的grouping信息）；我要给哪些task发送信息（同样根据groping信息）。 等所有的tuple都发送完了之后，CoordinateBolt通过另外一个特殊的stream以emitDirect的方式告诉所有它发送过 tuple的task，它发送了多少tuple给这个task。下游task会将这个数字和自己已经接收到的tuple数量做对比，如果相等，则说明处理 完了所有的tuple。 下游CoordinateBolt会重复上面的步骤，通知其下游。]]></content>
      <tags>
        <tag>
- storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm可靠性处理]]></title>
    <url>%2F2016%2F10%2F07%2Fstormke-kao-xing-chu-li%2F</url>
    <content type="text"><![CDATA[##Storm可靠性处理 spout可靠性处理 bolt可靠性处理 ###spout可靠性处理以单词计算的例子为例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package org.pslland;import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.IRichBolt;import org.apache.storm.topology.IRichSpout;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichSpout;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Values;import org.apache.storm.utils.Utils;import java.util.Map;import java.util.UUID;import java.util.concurrent.ConcurrentHashMap;/** * Created by psl on 10/5/16. */public class SentenceSpout extends BaseRichSpout &#123; private SpoutOutputCollector spoutOutputCollector = null; private ConcurrentHashMap&lt;UUID, Values&gt; pending; private String [] sentence = &#123; &quot;my dog has fleas&quot;, &quot;I like cold beverages&quot;, &quot;the dog ate my homework&quot;, &quot;don&apos;t have a cow man&quot;, &quot;i don&apos;t think i like fleas&quot; &#125;; private int index = 0; public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(&quot;sentence&quot;)); &#125; public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) &#123; this.spoutOutputCollector = spoutOutputCollector; this.pending = new ConcurrentHashMap&lt;UUID, Values&gt;(); &#125; public void nextTuple() &#123; UUID msgId = UUID.randomUUID(); ///发送时必须带上msgId this.spoutOutputCollector.emit(new Values(sentence[index]), msgId); this.pending.put(msgId, new Values(sentence[index])); index++; if(index &gt;= sentence.length)&#123; index = 0; &#125; Utils.sleep(1000); &#125; @Override public void ack(Object msgId) &#123; System.err.println(&quot;spout ack&quot;); this.pending.remove(msgId); &#125; @Override public void fail(Object msgId) &#123; this.spoutOutputCollector.emit(this.pending.get(msgId), msgId); &#125;&#125; ###bolt可靠性处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344package org.pslland;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;import java.util.Map;/** * Created by psl on 10/5/16. */public class SplitSentenceBolt extends BaseRichBolt &#123; private OutputCollector outputCollector = null; public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) &#123; this.outputCollector = outputCollector; &#125; public void execute(Tuple tuple) &#123; String sentence = tuple.getStringByField(&quot;sentence&quot;); String [] words = sentence.split(&quot; &quot;); System.out.println(&quot;splitSentence : &quot; + sentence); for (String word : words)&#123; //发送时必须带上 tuple this.outputCollector.emit(tuple, new Values(word)); &#125; this.outputCollector.ack(tuple); System.err.println(&quot;split ack&quot;); &#125; public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(&quot;word&quot;)); &#125;&#125; 后面接着的bolt必须全部都加上tuple向后发送，并且处理完成后，显示的调用ack方法。]]></content>
      <tags>
        <tag>
- storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm单词计数示例]]></title>
    <url>%2F2016%2F10%2F07%2Fstormdan-ci-ji-shu-shi-li%2F</url>
    <content type="text"><![CDATA[##Storm单词计数示例 top代码 spout代码 bolt代码 遇到的bug ###top代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package org.pslland;import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.StormSubmitter;import org.apache.storm.generated.AlreadyAliveException;import org.apache.storm.generated.AuthorizationException;import org.apache.storm.generated.InvalidTopologyException;import org.apache.storm.topology.TopologyBuilder;import org.apache.storm.tuple.Fields;import org.apache.storm.utils.Utils;/** * Created by psl on 10/6/16. */public class WordCountTopology &#123; private static final String SENTENCE_SPOUT_ID = &quot;sentence-spout&quot;; private static final String SPLIT_BOLT_ID = &quot;split-bolt&quot;; private static final String COUNT_BOLT_ID = &quot;count-bolt&quot;; private static final String REPORT_BOLT_ID = &quot;report-bolt&quot;; private static final String TOPOLOGY_NAME = &quot;word-count-topology&quot;; public static void main(String [] args) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException &#123; SentenceSpout sentenceSpout = new SentenceSpout(); SplitSentenceBolt splitSentenceBolt = new SplitSentenceBolt(); WordCountBolt wordCountBolt = new WordCountBolt(); ReportBolt reportBolt = new ReportBolt(); TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(SENTENCE_SPOUT_ID, sentenceSpout);//, 2); builder.setBolt(SPLIT_BOLT_ID, splitSentenceBolt, 2).setNumTasks(4).shuffleGrouping(SENTENCE_SPOUT_ID); //builder.setBolt(SPLIT_BOLT_ID, splitSentenceBolt).shuffleGrouping(SENTENCE_SPOUT_ID); builder.setBolt(COUNT_BOLT_ID, wordCountBolt, 6).fieldsGrouping(SPLIT_BOLT_ID, new Fields(&quot;word&quot;)); //builder.setBolt(COUNT_BOLT_ID, wordCountBolt).fieldsGrouping(SPLIT_BOLT_ID, new Fields(&quot;word&quot;)); builder.setBolt(REPORT_BOLT_ID, reportBolt).globalGrouping(COUNT_BOLT_ID); Config config = new Config(); //config.setNumWorkers(2); if (args.length == 0) &#123; LocalCluster cluster = new LocalCluster(); cluster.submitTopology(TOPOLOGY_NAME, config, builder.createTopology()); //StormSubmitter.submitTopology(TOPOLOGY_NAME, config, builder.createTopology()); Utils.sleep(20000); cluster.killTopology(TOPOLOGY_NAME); cluster.shutdown(); &#125;else &#123; StormSubmitter.submitTopology(args[0], config, builder.createTopology()); &#125; &#125;&#125; ###spout代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package org.pslland;import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.IRichBolt;import org.apache.storm.topology.IRichSpout;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichSpout;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Values;import org.apache.storm.utils.Utils;import java.util.Map;/** * Created by psl on 10/5/16. */public class SentenceSpout extends BaseRichSpout &#123; private SpoutOutputCollector spoutOutputCollector = null; private String [] sentence = &#123; &quot;my dog has fleas&quot;, &quot;I like cold beverages&quot;, &quot;the dog ate my homework&quot;, &quot;don&apos;t have a cow man&quot;, &quot;i don&apos;t think i like fleas&quot; &#125;; private int index = 0; public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(&quot;sentence&quot;)); &#125; public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) &#123; this.spoutOutputCollector = spoutOutputCollector; &#125; public void nextTuple() &#123; this.spoutOutputCollector.emit(new Values(sentence[index])); index++; if(index &gt;= sentence.length)&#123; index = 0; &#125; Utils.sleep(1000); &#125;&#125; ###bolt代码splitSentenceBolt.java 123456789101112131415161718192021222324252627282930313233343536373839package org.pslland;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;import java.util.Map;/** * Created by psl on 10/5/16. */public class SplitSentenceBolt extends BaseRichBolt &#123; private OutputCollector outputCollector = null; public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) &#123; this.outputCollector = outputCollector; &#125; public void execute(Tuple tuple) &#123; String sentence = tuple.getStringByField(&quot;sentence&quot;); String [] words = sentence.split(&quot; &quot;); System.out.println(&quot;splitSentence : &quot; + sentence); for (String word : words)&#123; this.outputCollector.emit(new Values(word)); &#125; &#125; public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(&quot;word&quot;)); &#125;&#125; WordCountBolt.java 123456789101112131415161718192021222324252627282930313233343536373839404142package org.pslland;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;import java.util.HashMap;import java.util.Map;/** * Created by psl on 10/5/16. */public class WordCountBolt extends BaseRichBolt &#123; private OutputCollector outputCollector = null; private HashMap&lt;String, Long&gt; counts = null; public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) &#123; this.outputCollector = outputCollector; this.counts = new HashMap&lt;String, Long&gt;(); &#125; public void execute(Tuple tuple) &#123; String word = tuple.getStringByField(&quot;word&quot;); Long count = this.counts.get(word); if(count == null)&#123; count = 0L; &#125; count++; this.counts.put(word, count); this.outputCollector.emit(new Values(word, count)); &#125; public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(&quot;word&quot;, &quot;count&quot;)); &#125;&#125; ReportBolt.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package org.pslland;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Tuple;import java.util.*;/** * Created by psl on 10/5/16. */public class ReportBolt extends BaseRichBolt &#123; private HashMap&lt;String, Long&gt; counts = null; public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) &#123; this.counts = new HashMap&lt;String, Long&gt;(); &#125; public void execute(Tuple tuple) &#123; String word = tuple.getStringByField(&quot;word&quot;); Long count = tuple.getLongByField(&quot;count&quot;); this.counts.put(word, count); //this.cleanup(); &#125; public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; //last bolt &#125; public void cleanup()&#123; System.out.println(&quot;----------- FINAL COUNTS -----------&quot;); List&lt;String&gt; keys = new ArrayList&lt;String&gt;(); keys.addAll(this.counts.keySet()); Collections.sort(keys); for (String key : keys)&#123; System.out.println(key + &quot; : &quot; + this.counts.get(key)); &#125; System.out.println(&quot;-----------------------------------&quot;); &#125;&#125; ###遇到的bug我的代码是通过IDEA打包的。相关maven配置： 12345&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.pslland&lt;/groupId&gt; &lt;artifactId&gt;stormWC&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-core&lt;/artifactId&gt; &lt;version&gt;1.0.2&lt;/version&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 本地调试的时候去掉provided标签。 在集群提交测试的时候如果编译没有加上provided 标签会产生如下错误：Caused by: java.io.IOException: Found multiple defaults.yaml resources. 集群提交命令：./storm jar 包名 类名 参数]]></content>
      <tags>
        <tag>
- storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm分组方式和并发]]></title>
    <url>%2F2016%2F10%2F07%2Fstormfen-zu-fang-shi-he-bing-fa%2F</url>
    <content type="text"><![CDATA[##Storm分组方式和并发 基础理论 Storm分组方式 Storm并发 ###基础理论 Nodes(服务器):指配置在一个Storm集群中的服务器，会执行top的一部分运算。一个Storm集群可以包括一个或者多个工作node。 Worker(JVM虚拟机): 指一个node上相互独立运行的JVM进程。每一个node可以配置运行一个或者多个worker。一个top会分配到一个或者多个worker上运行。 Executor(线程):指一个worker的JVM进程中运行的Java线程。多个task可以指派给同一个executor来执行。除非是明确指定，Storm默认会给每一个executor分配一个task。 Task(Spout/bolt实例):task是spout和bolt的实例，它们的nextTuple（）和execute（）方法会被executor线程调用执行。task代表最大并发度。executor数代表实际并发数。 ###Storm并发并发举例 Config conf = new Config();//设置两个workerconf.setNumWorkers(2);//spout数目线程数2topologyBuilder.setSpout(“blue-spout”, new BlueSpout(),2);//线程数为2，并发度为4 见图topologyBuilder.setBolt(“green-bolt”, new GreenBolt(), 2).setNumTasks(4).shuffleGrouping(“blue-spout”);//线程数为6topologyBuilder.setBolt(“yellow-bolt”, new YellowBolt(), 6).shuffleGrouping(“green-bolt”);StormSubmitter.submitTopology( “mytopology”, conf, topologyBuilder.createTopology() ); ###Storm分组方式数据流分组定义了一个数据流中的tuple如何发送给top中不同bolt的task。注：不是一个spout或bolt emit到多个bolt（广播方式）目前官方有八种 Shuffle Grouping(随机分组)：这种方式会随机分发tuple给bolt的各个task，每个bolt实例接收到的相同数量的tuple。 Field Grouping(按字段分组)：根据指定的字段进行分组。（重点） Non Grouping: 无分组， 这种分组和Shuffle grouping是一样的效果，多线程下不平均分配。 All Grouping： 广播发送。The stream is replicated across all the bolt’s tasks. Use this grouping with care. Global Grouping: 全局分组， 这个tuple被分配到storm中的一个bolt的其中一个task。再具体一点就是分配给id值最低的那个task。适合场景：想象不到。 Direct Grouping: 直接分组， 这是一种比较特别的分组方法，用这种分组意味着消息的发送者决定由消息接收者的哪个task处理这个消息。 只有被声明为Direct Stream的消息流可以声明这种分组方法。而且这种消息tuple必须使用emitDirect方法来发射。消息处理者可以通过TopologyContext来或者处理它的消息的taskid (OutputCollector.emit方法也会返回taskid) Partial Key grouping: The stream is partitioned by the fields specified in the grouping, like the Fields grouping, but are load balanced between two downstream bolts, which provides better utilization of resources when the incoming data is skewed. This paper provides a good explanation of how it works and the advantages it provides.（最新新增） Local or shuffle grouping: If the target bolt has one or more tasks in the same worker process, tuples will be shuffled to just those in-process tasks. Otherwise, this acts like a normal shuffle grouping.]]></content>
      <tags>
        <tag>
- storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm示例程序]]></title>
    <url>%2F2016%2F10%2F05%2Fstormshi-li-cheng-xu%2F</url>
    <content type="text"><![CDATA[##Storm示例程序 测试数据 Spout程序 Bolt程序 主程序 ###测试数据由于是简单的示例测试程序，我直接弄了一个代码生成了相关的数据文件。最后文件数据的格式如下：www.pslland.org CYYH6Y2345GHI899OFG4V9U567 2014-01-07 11:40:49www.taobao.com XXYH6YCGFJYERTT834R52FDXV9U34 2014-01-07 08:40:50www.taobao.com CYYH6Y2345GHI899OFG4V9U567 2014-01-07 09:40:49www.taobao.com BBYH61456FGHHJ7JL89RG5VV9UYU7 2014-01-07 08:40:53www.taobao.com VVVYH6Y4V4SFXZ56JIPDPB4V678 2014-01-07 10:40:49www.taobao.com ABYH6Y4V4SCVXTG6DPB4VH9U123 2014-01-07 08:40:50www.taobao.com CYYH6Y2345GHI899OFG4V9U567 2014-01-07 11:40:49www.pslland.org XXYH6YCGFJYERTT834R52FDXV9U34 2014-01-07 08:40:52行数自己控制 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.io.File;import java.io.FileOutputStream;import java.io.IOException;import java.util.Random;/** * Created by psl on 9/24/16. */public class genrateData &#123; public static void main(String [] argv)&#123; File logFile = new File(&quot;test.log&quot;); Random random = new Random(); String [] hosts = &#123;&quot;www.pslland.org&quot;, &quot;www.taobao.com&quot;&#125;; String [] sessions = &#123;&quot;ABYH6Y4V4SCVXTG6DPB4VH9U123&quot;, &quot;XXYH6YCGFJYERTT834R52FDXV9U34&quot;, &quot;BBYH61456FGHHJ7JL89RG5VV9UYU7&quot;, &quot;CYYH6Y2345GHI899OFG4V9U567&quot;, &quot;VVVYH6Y4V4SFXZ56JIPDPB4V678&quot;&#125;; String [] time = &#123;&quot;2014-01-07 08:40:50&quot;, &quot;2014-01-07 08:40:51&quot;, &quot;2014-01-07 08:40:52&quot;, &quot;2014-01-07 08:40:53&quot;, &quot;2014-01-07 09:40:49&quot;, &quot;2014-01-07 10:40:49&quot;, &quot;2014-01-07 11:40:49&quot;, &quot;2014-01-07 12:40:49&quot;&#125;; StringBuffer stringBuffer = new StringBuffer(); for (int i = 0; i &lt; 50; i++)&#123; stringBuffer.append(hosts[random.nextInt(2)] + &apos;\t&apos; + sessions[random.nextInt(5)] + &apos;\t&apos; + time[random.nextInt(8)] + &apos;\n&apos;); &#125; if (!logFile.exists())&#123; try &#123; logFile.createNewFile(); &#125;catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; FileOutputStream fs = null; byte [] b = (stringBuffer.toString()).getBytes(); try &#123; fs = new FileOutputStream(logFile); fs.write(b); fs.close(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125; ###Spout程序 类继承图 ISpout接口 Spout程序层抽象是ISpout接口Open()是初始化方法nextTuple()循环发射数据ack() 成功处理tuple回调方法Fail()处理失败tuple回调方法activate和deactivate ：spout可以被暂时激活和关闭close方法在该spout关闭前执行，但是并不能得到保证其一定被执行，kill -9时不执行，Storm kill {topoName} 时执行 原则：通常情况下（Shell和事务型的除外），实现一个Spout，可以直接实现接口IRichSpout，如果不想写多余的代码，可以直接继承BaseRichSpout。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.IRichSpout;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Values;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.InputStreamReader;import java.util.Map;/** * Created by psl on 9/24/16. */public class mySpout implements IRichSpout&#123; public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(&quot;LOG&quot;)); &#125; public Map&lt;String, Object&gt; getComponentConfiguration() &#123; return null; &#125; SpoutOutputCollector spoutOutputCollector = null; FileInputStream fs = null; InputStreamReader isr = null; BufferedReader br; public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) &#123; try &#123; this.spoutOutputCollector = spoutOutputCollector; this.fs = new FileInputStream(&quot;test.log&quot;); this.isr = new InputStreamReader(fs, &quot;UTF-8&quot;); this.br = new BufferedReader(isr); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; public void close() &#123; try &#123; this.br.close(); this.isr.close(); this.fs.close(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; public void activate() &#123; &#125; public void deactivate() &#123; &#125; String str = null; public void nextTuple() &#123; try &#123; while ((str = this.br.readLine()) != null)&#123; spoutOutputCollector.emit(new Values(str)); Thread.sleep(3000); &#125; &#125;catch ( Exception e)&#123; e.printStackTrace(); &#125; &#125; public void ack(Object o) &#123; System.out.println(&quot;spout ack: &quot; + o.toString()); &#125; public void fail(Object o) &#123; System.out.println(&quot;spout fail: &quot; + o.toString()); &#125;&#125; ###Bolt程序 Bolt类继承图 IBolt方法 prepare方法进行初始化，传入当前执行的上下文execute接受一个tuple进行处理，也可emit数据到下一级组件cleanup 同ISpout的close方法，在关闭前调用，不保证其一定执行。 IBolt继承了Serializable，我们在nimbus上提交了topology以后，创建出来的bolt会序列化后发送到具体执行的worker(工作进程)上去。worker在执行该Bolt时，会先调用prepare方法传入当前执行的上下文.execute接受一个tuple进行处理，并用prepare方法传入的OutputCollector的ack方法（表示成功）或fail（表示失败）来反馈处理结果.还可以通过OutputCollector的emit方法把结果发射到下一级组件。 IBasicBolt接口，实现该接口的Bolt不用在代码中提供反馈结果了，Storm内部会自动反馈成功。如果你确实要反馈失败，可以抛出FailedException。 （后续章节讲到的Trident均不用显性调用ack和fail，框架会自动调。） 实现一个Bolt，可以实现IRichBolt接口或继承BaseRichBolt，如果不想自己处理结果反馈，可以实现 IBasicBolt接口或继承BaseBasicBolt，它实际上相当于自动做了prepare方法和 collector.emit.ack(inputTuple)。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import org.apache.storm.task.IBolt;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.IRichBolt;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import java.util.Map;/** * Created by psl on 9/24/16. */public class mybolt implements IRichBolt&#123; OutputCollector outputCollector = null; public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) &#123; this.outputCollector = outputCollector; &#125; String valueString = null; int num = 0; public void execute(Tuple tuple) &#123; try &#123; valueString = tuple.getStringByField(&quot;LOG&quot;); if(valueString != null)&#123; num++; System.err.println(Thread.currentThread().getName() + &quot; line : &quot; + num + &quot; sessionId : &quot; + valueString.split(&quot;\t&quot;)[1]); &#125; outputCollector.ack(tuple); Thread.sleep(2000); &#125;catch (Exception e)&#123; outputCollector.fail(tuple); e.printStackTrace(); &#125; &#125; public void cleanup() &#123; &#125; public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(&quot;&quot;)); &#125; public Map&lt;String, Object&gt; getComponentConfiguration() &#123; return null; &#125;&#125; ###主程序 123456789101112131415161718192021222324252627282930313233import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.StormSubmitter;import org.apache.storm.topology.TopologyBuilder;/** * Created by psl on 9/24/16. */public class main &#123; public static void main(String [] argv)&#123; TopologyBuilder topologyBuilder = new TopologyBuilder(); topologyBuilder.setSpout(&quot;spout&quot;, new mySpout(), 1); topologyBuilder.setBolt(&quot;bolt&quot;, new mybolt(), 1).shuffleGrouping(&quot;spout&quot;); Config conf = new Config() ; conf.setDebug(true); if (argv.length &gt; 0)&#123; try &#123; StormSubmitter.submitTopology(argv[0], conf, topologyBuilder.createTopology()); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;else &#123; LocalCluster localCluster = new LocalCluster(); localCluster.submitTopology(&quot;mytopology&quot;, conf, topologyBuilder.createTopology()); &#125; &#125;&#125;]]></content>
      <tags>
        <tag>
- storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase协处理器]]></title>
    <url>%2F2016%2F10%2F04%2Fhbasexie-chu-li-qi%2F</url>
    <content type="text"><![CDATA[##Hbase 协处理器HBase 做为列数据库最令人诟病的特性包括：无法轻易建立‘二级索引’，难以执行求和、计数、排序等操作。 为了减少在客服端做大量的运算，Hbase引入了coprocessor，能够轻易建立二次索引、复杂过滤器以及访问控制符等。 协处理器分为两种类型，系统协处理器可以全局导入region server上所有的数据表，表协处理器即是用户可以指定一张表使用协处理器。协处理器为了提高灵活性，提供了两种不同的插件。一个是观察者模式（observer），类似与数据库的触发器。另一个是终端（endpoint）,动态的终端有点像存储过程。 observer endpoint ###observer模型 以上的函数是比较老的，注意最新的代码。 Observer最新的类型有四种： RegionObserver: 提供客户端的数据操纵事件钩子:Get、Put、Delete、Scan等。 RegionServerObserver : A RegionServerObserver allows you to observe events related to the RegionServer’s operation, such as starting, stopping, or performing merges, commits, or rollbacks. 最新加的。 MasterObserver： 提供DDL-类型的操作钩子。如创建、删除、修改数据表等。 这些接口可以同时使用在同一个地方,按照不同优先级顺序执行.用户可以任意基于协处理 器实现复杂的HBase功能层。HBase有很多种事件可以触发观察者方法,这些事件与方法 从HBase0.92版本起,都会集成在HBase API中。不过这些API可能会由于各种原因有所改 动,不同版本的接口改动比较大。 WalObserver：提供WAL相关操作钩子。 主要讨论RegionObserver相关代码的编写： public class RegionObserverTest extends BaseRegionObserver{ private static byte[] fixed_rowkey = Bytes.toBytes(&quot;0004&quot;); @Override public void preGetOp(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Get get, List&lt;Cell&gt; results) throws IOException { if(Bytes.equals(get.getRow(), fixed_rowkey)){ KeyValue keyValue = new KeyValue(get.getRow(), Bytes.toBytes(&quot;time&quot;), Bytes.toBytes(&quot;key_changed&quot;), Bytes.toBytes(&quot;value_changed&quot;)); results.add(keyValue); } } } 在编写代码中遇到的问题： 1.IDEA找不到BaseRegionObserver。这个是因为原先我们编写Hbase客服端程序只用到了hbase-client包，在编写coprocessor程序的时候需要导入hbase-server. \&lt;dependency&gt; \&lt;groupId&gt;org.apache.hbase\&lt;/groupId&gt; \&lt;artifactId&gt;hbase-server\&lt;/artifactId&gt; \&lt;version&gt;1.2.3\&lt;/version&gt; \&lt;/dependency&gt; 2.程序正常导入表之后，scan出来的行没有变化。这个是因为此coprocessor只是针对get方法，使用get去获取某一行就OK。 如何使其正常运作。 先将上述的代码打包生成一个jar文件。 将jar文件传入HDFS的一个相应的路径 disable 表 （disable ‘user_info’） alter 修改表相关配置 （alter ‘user_info’, ‘coprocessor’=&gt;’hdfs:///coprocessor/hbaseRegionObserver.jar|org.pslland.RegionObserverTest||’） enable相关表 （enable ‘user_info’） get相关数据，检查是否生效 （get ‘user_info’, ‘0004’） ###endpointendpoint常用于求和等高级功能，相关API在1.2.3里面改动比较大。相关例子参考：http://www.3pillarglobal.com/insights/hbase-coprocessors]]></content>
      <tags>
        <tag>
- Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase过滤器的使用]]></title>
    <url>%2F2016%2F10%2F04%2Fhbaseguo-lu-qi-de-shi-yong%2F</url>
    <content type="text"><![CDATA[##HBsae 过滤器 过滤器API ###过滤器的使用 使用过滤器可以提高操作表的效率,HBase中两种数据读取函数get()和scan()都支持过滤器,支持直接访问和通过指定起止行键来访问,但是缺少细粒度的筛选功能,如基于正则表达式对行键或值进行筛选的功能。 可以使用预定义好的过滤器或者是实现自定义过滤器 过滤器在客户端创建,通过RPC传送到服务器端,在服务器端执行过滤操作,把数据返回给客户端 原理图 不使用过滤器的情况下，scan可以通过设置开始行和结束行的方式进行过滤。（其中设置的条件是左闭右开）scan ‘user_info’, {STARTROW=&gt;’0001’, ENDROW=&gt;’0003’}缺少细粒度的筛选功能 ####过滤器类型和名称 Comparision Filters(比较过滤器) (主要针对行键名和列族名进行过滤) RowFilter FamilyFilter QualifierFilter ValueFilter DependentColumnFilter Dedicated Filters(专用过滤器) （可以针对值进行过滤） SingleColumnValueFilter SingleColumnValueExcludeFilter PrefixFilter PageFilter KeyOnlyFilter FirstKeyOnlyFilter TimestampsFilter RandomRowFilter 3.Decorating Filters(附加过滤器)SkipFilterWhileMatchFilters ####过滤器的使用RowFilter—&gt;BinaryComparator,RegexStringComparator public static void filterTest(String tableName, Connection connection){ Scan scan = new Scan(); scan.setCaching(1000); ////BinaryComparator, BinaryPrefixComparator, BitComparator, LongComparator, NullComparator, RegexStringComparator, SubstringComparator // RowFilter rowFilter = new RowFilter(CompareFilter.CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes(&quot;0001&quot;))); RowFilter rowFilter = new RowFilter(CompareFilter.CompareOp.EQUAL, new RegexStringComparator(&quot;000\\w+&quot;)); scan.setFilter(rowFilter); System.out.println(&quot;test-----&quot;); try { Table table = connection.getTable(TableName.valueOf(tableName)); ResultScanner scanner = table.getScanner(scan); for (Result result: scanner){ System.out.println(result); } } catch (IOException e) { e.printStackTrace(); } } PageFilter public static void PagefilterTest(String tableName, Connection connection){ //scan.setCaching(1000); PageFilter pageFilter = new PageFilter(3); byte [] lastRow = null; int pageCount = 0; try { while (++pageCount &gt; 0){ System.out.println(&quot;pageCount: &quot; + pageCount); Scan scan = new Scan(); scan.setFilter(pageFilter); if (lastRow != null){ scan.setStartRow(lastRow); } Table table = connection.getTable(TableName.valueOf(tableName)); ResultScanner scanner = table.getScanner(scan); int count = 0; for (Result result: scanner){ lastRow = result.getRow(); //避免每一页最后一个row重复 if(++count &gt;= 3){ break; } System.out.println(result); } ////处理最后一页 if(count &lt; 3){ break; } } } catch (IOException e) { e.printStackTrace(); } }]]></content>
      <tags>
        <tag>
- Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase基础API的使用]]></title>
    <url>%2F2016%2F10%2F03%2Fhbaseji-chu-apide-shi-yong%2F</url>
    <content type="text"><![CDATA[##HBsae API使用 基础API 扫描器API ###基础API 与 扫描器API1.创建表 Configuration conf = new Configuration(); conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;192.168.0.110:2181&quot;); Connection conn = null; conn = ConnectionFactory.createConnection(conf); HBaseAdmin hBaseAdmin = (HBaseAdmin)conn.getAdmin(); createTable(&quot;tableName1&quot;, &quot;tst1&quot;, &quot;tst2&quot;, hBaseAdmin); public static void createTable(String tableName, String column1, String column2, HBaseAdmin hBaseAdmin)throws IOException{ //TableName tableName = new TableName(); HTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf(tableName)); tableDescriptor.addFamily(new HColumnDescriptor(column1)); HColumnDescriptor hColumnDescriptor = new HColumnDescriptor(column2); hColumnDescriptor.setVersions(3, 3); tableDescriptor.addFamily(hColumnDescriptor); hBaseAdmin.createTable(tableDescriptor); } 2.插入数据 Configuration conf = new Configuration(); conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;192.168.0.110:2181&quot;); //HBaseAdmin hBaseAdmin = new HBaseAdmin(conf); Connection conn = null; conn = ConnectionFactory.createConnection(conf); putDatas(&quot;tableName1&quot;, conn); public static void putDatas(String tableName, Connection connection) throws IOException{ //String [] rows = Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(&quot;rowKey1_psl01&quot;.getBytes()); put.addColumn(&quot;tst1&quot;.getBytes(), &quot;tst1_key&quot;.getBytes(), &quot;tst1_value&quot;.getBytes()); put.addColumn(&quot;tst2&quot;.getBytes(), &quot;tst2_key&quot;.getBytes(), &quot;tst2_value&quot;.getBytes()); table.put(put); } 3.获取数据 Configuration conf = new Configuration(); conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;192.168.0.110:2181&quot;); //HBaseAdmin hBaseAdmin = new HBaseAdmin(conf); Connection conn = null; conn = ConnectionFactory.createConnection(conf); getData(&quot;tableName1&quot;, conn); public static void getData(String tableName, Connection connection) throws IOException{ Get get = new Get(&quot;rowKey1_psl01&quot;.getBytes()); get.addColumn(&quot;tst1&quot;.getBytes(), &quot;tst1_key&quot;.getBytes()); Table table = connection.getTable(TableName.valueOf(tableName)); Result result = table.get(get); List&lt;Cell&gt; cells = result.listCells(); for (Cell cell : cells){ System.out.println(&quot;qualifier : &quot; + new String(CellUtil.cloneQualifier(cell))); System.out.println(&quot;value: &quot; + new String(CellUtil.cloneValue(cell))); } } 4.删除数据 public static void delData(String tableName, Connection connection) throws IOException{ //hBaseAdmin.disableTable(); Delete delete = new Delete(&quot;rowKey1_psl01&quot;.getBytes()); delete.addColumn(&quot;tst1&quot;.getBytes(), &quot;tst1_key&quot;.getBytes()); Table table = connection.getTable(TableName.valueOf(tableName)); table.delete(delete); } 5.扫描表 public static void hbaseScan(String tableName, Connection connection){ Scan scan = new Scan(); scan.setCaching(1000); try { Table table = connection.getTable(TableName.valueOf(tableName)); ResultScanner scanner = table.getScanner(scan); for (Result result: scanner){ System.out.println(result); } } catch (IOException e) { e.printStackTrace(); } }]]></content>
      <tags>
        <tag>
- Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase读数据流程]]></title>
    <url>%2F2016%2F09%2F27%2Fhbasedu-shu-ju-liu-cheng%2F</url>
    <content type="text"><![CDATA[##Hbase读数据 Hbase读数据流程*Hbase -ROOT- 和 .META.表结构 ###Hbase读数据流程Hbase读数据流程图 1.从zk里面获取-ROOT-表所在regionServer的信息。 2.访问-ROOT-表所在的RegionServer，找到所要查询表的记录的二级索引。 3.根据-ROOT-表里面的记录，查询对应的.META.表的regionServer，获取表真实记录所在的regionServer信息。 4.根据在.MATE.所获取的信息，查询表所在的regionServer，并获取相应的数据。 ###Hbase -ROOT- 和 .MATE.表结构 ROOT表结构 META表结构 两张表的结构类似，因为都是用于定位表的位置。仔细看看两张表的ROWKEY的定义，对表的位置定位基本上就清楚啦。]]></content>
      <tags>
        <tag>
- Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase存储原理和基础命令]]></title>
    <url>%2F2016%2F09%2F25%2Fhbasecun-chu-yuan-li-he-ji-chu-ming-ling%2F</url>
    <content type="text"><![CDATA[##Hbase原理和基础命令 Hbase存储基本原理 Client端基础命令 ###Hbase存储基本原理 存储原理图 大表存储图 数据管理机制1、hbase中的表很大—bigtable，都是分布式存储在集群的各个regionserver上 2、分布式存储时，需要对表进行切分，首先是按行切分成若干个hregion 3、表的每一个hregion都会被一个regionserver所管理 4、每一个hregion随着插入数据的增多，一旦达到一个阈值，会被regionserver分裂成两个 5、在一个hregion内部还会被按照列族切分成若干个store单元 6、每一个store又会被切分成若干个store file(HFile)存储到hdfs文件系统中 7、每一个store对象中会维护一个内存缓存 memstore，用户查询数据时首先会去memstore中进行命中 8、客户端往表中插入数据，首先会在hlog日志中进行记录，然后定期进行flush合并 ###Client端基础语法 Hbase基础表结构 1.创建表的基础命令 create ‘user_info’, ‘base_info’, ‘extra_info’; 指定版本号： create ‘user_info’, {NAME =&gt;’base_info’, VERSION =&gt; 3}, ‘extra_info’ 2.插入数据 put ‘user_info’, ‘0001’, ‘base_info:phone’, ‘13967109402’ put ‘user_info’, ‘0001’, ‘base_info:name’, ‘angleababy’ 3.全表扫描 scan ‘user_info’ 4.查看数据 get ‘user_info’,’0001’, ‘base_info:name’, ‘base_info:age’ get ‘user_info2’, ‘0001’, { COLUMN =&gt; ‘base_info:name’, VERSIONS =&gt; 4} 查看多个版本的数据 5.查看表的名称：list 6.Hbase中数据库的概念，namespaces 创建一个命名空间my_ns create_namespace &apos;my_ns&apos; 在命名空间my_ns里创建my_table create &apos;my_ns:my_table&apos;, &apos;fam&apos; drop namespace drop_namespace &apos;my_ns&apos; alter namespace alter_namespace &apos;my_ns&apos;, {METHOD =&gt; &apos;set&apos;, &apos;PROPERTY_NAME&apos; =&gt; &apos;PROPERTY_VALUE&apos;} 默认的namespace： default:一般没有定义名称空间的表都在default里面。 hbase:里面存放的是Hbase的基础表（root,meta,namespace）]]></content>
      <tags>
        <tag>
- Hbase
- Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm基础原理]]></title>
    <url>%2F2016%2F09%2F24%2Fstormji-chu-yuan-li%2F</url>
    <content type="text"><![CDATA[##Storm原理图 主架构 主流程 ###主架构 主节点 工作节点 作业 Storm Nimbus supervisor topologies,死循环 hadoop Jobtracker Tasktracker MapReduce Job，执行完自动结束 主要架构图 Nimbus 和supervisor之间所有的协调工作时通过一个zookeeper集群。Nimbus 进程和supervisor进程是无法直接连接和无状态的；所有的状态维持在zookeeper中或保持在本地磁盘上 Nimbus 负责在集群分发代码，top只能在Nimbus机器上面提交，将任务分配给其他机器。Supervisor监听分配给它的节点，根据Nimbus的委派在必要时启动和关闭工作进程。每个工作进程执行top的一个子集。一个运行中的top由很多运行在很多机器上的工作进程组成。在Storm中有对于流stream的抽象。流是一个不间断的无界的连续的tuple，注意Storm在建模事件流时，把流中德事件抽象为tuple即元组 工作图 ###主流程Storm认为每个stream都有一个源，也就是原始元组的源头，叫做Spout（管口） 处理stream内的tuple，抽象为Bolt，bolt可以消费任意数量的输入流，只要将流方向导向该bolt，同时它也可以发送新的流给其他bolt使用，这样一来，只要打开特定的spout再将spout中流出的tuple导向特定的bolt，又bolt对导入的流做处理后再导向其他bolt或者目的地。 可以认为spout就是水龙头，并且每个水龙头里流出的水是不同的，我们想拿到哪种水就拧开哪个水龙头，然后使用管道将水龙头的水导向到一个水处理器（bolt），水处理器处理后再使用管道导向另一个处理器或者存入容器中。 spout和bolt工作图 这是一张有向无环图，Storm将这个图抽象为Topology(拓扑)，Topo就是storm的Job抽象概念，一个拓扑就是一个流转换图 图中每个节点是一个spout或者bolt，每个spout或者bolt发送元组到下一级组件，广播方式。 而Spout到单个Bolt有6种grouping方式，后续细讲。 Storm将流中元素抽象为tuple，一个tuple就是一个值列表value list，list中的每个value都有一个name，并且该value可以是任意可序列化的类型。拓扑的每个节点都要说明它所发射出的元组的字段的name，其他节点只需要订阅该name就可以接收处理。 Streams：消息流消息流是一个没有边界的tuple序列，而这些tuples会被以一种分布式的方式并行创建和处理。 每个tuple可以包含多列，字段类型可以是： integer, long, short, byte, string, double, float, boolean和byte array。 你还可以自定义类型 — 只要你实现对应的序列化器。 Spouts：消息源Spouts是topology消息生产者。Spout从一个外部源(消息队列)读取数据向topology发出tuple。 消息源Spouts可以是可靠的也可以是不可靠的。一个可靠的消息源可以重新发射一个处理失败的tuple， 一个不可靠的消息源Spouts不会。 Spout类的方法nextTuple不断发射tuple到topology，storm在检测到一个tuple被整个topology成功处理的时候调用ack, 否则调用fail。storm只对可靠的spout调用ack和fail。 Bolts：消息处理者消息处理逻辑被封装在bolts里面，Bolts可以做很多事情： 过滤， 聚合， 查询数据库等。Bolts可以简单的做消息流的传递。复杂的消息流处理往往需要很多步骤， 从而也就需要经过很多Bolts。第一级Bolt的输出可以作为下一级Bolt的输入。而Spout不能有一级。 Bolts的主要方法是execute（死循环）连续处理传入的tuple，成功处理完每一个tuple调用OutputCollector的ack方法，以通知storm这个tuple被处理完成了。当处理失败时，可以调fail方法通知Spout端可以重新发送该tuple。 流程是： Bolts处理一个输入tuple, 然后调用ack通知storm自己已经处理过这个tuple了。storm提供了一个IBasicBolt会自动调用ack。Bolts使用OutputCollector来发射tuple到下一级Blot。]]></content>
      <tags>
        <tag>
- storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase集群搭建]]></title>
    <url>%2F2016%2F09%2F24%2Fhbaseji-qun-da-jian%2F</url>
    <content type="text"><![CDATA[##Hbase环境搭建 搭建zookeeper集群和HDFS集群 安装Hbase软件 启动集群 shell客户端连接 ###搭建zookeeper集群和HDFS集群zookeeper 对于Hbase集群是必不可少的。zookeeper为hadoop集群提供协同服务。HDFS为Hbase提供体层的存储服务zookeeper和HDFS搭建参考相关博文 ###安装Hbase软件目前Hbase最新稳定版本为1.2.3。在官网相关网页下载相关软件包。 1.将软件包解压到相关路径 进入配置文件目录，修改相关配置文件 hbase-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://Master:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hbase是分布式的 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk的地址，多个用“,”分割 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;Master:2181,Slave1:2181,Slave2:2181&lt;/value&gt; &lt;/property&gt; hbase-env.sh export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 export HBASE_OPTS=&quot;-XX:+UseConcMarkSweepGC&quot; export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m&quot; export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m&quot; export HBASE_MANAGES_ZK=false regionservers Slave1 Slave2 Slave3 Slave4 以上三个文件主要配置zookeeper和HDFS相关配置信息 ###启动集群1.启动集群进入软件包的bin目录，执行start-hbase.sh便可以启动Hbase集群。 2.web界面访问http://master:16010/master-status可以看到集群的状态 3.在HMaster上面有HMaster进程，在HRegion上面有一个HReginServer的进程。4.启动HMaster的backup进程。在其他机器上进入bin目录启动./hbase_daemon.sh start master ###shell客户端连接进入软件包的bin目录，运行./hbase shell即可进入Hbase的shell client端。]]></content>
      <tags>
        <tag>
- Hbase
- Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm集群搭建]]></title>
    <url>%2F2016%2F09%2F24%2Fstormji-qun-da-jian%2F</url>
    <content type="text"><![CDATA[##storm 集群搭建 zookeeper环境 Storm配置 Storm运行 ###zookeeper环境zookeeper环境搭建参考zookeeper搭建相关博文 ###Storm配置 1. 下载软件包 1. 在博主学习的时候，最新的storm版本是1.02。相关下载链接：http://storm.apache.org/releases/1.0.2/index.html 2. 解压到相关目录 tar -zxvf apache-storm-1.0.2.tar.gz 博主比较懒，直接解压在hadoop的主目录中。/home/hadoop/storm/apache-storm-1.0.2 3.配置相关文件 storm最主要的配置文件是：/home/hadoop/storm/apache-storm-1.0.2/conf/storm.yaml 相关配置如下： # Licensed to the Apache Software Foundation (ASF) under one storm.zookeeper.servers: - &quot;master&quot; - &quot;slave1&quot; - &quot;slave2&quot; nimbus.host: &quot;master&quot; nimbus的主机ip或者名称 nimbus.seeds: [&quot;master&quot;, &quot;slave1&quot;, &quot;slave2&quot;, &quot;slave3&quot;, &quot;slave4&quot;] 所有机器列表 storm.local.dir: &quot;/home/hadoop/storm/apache-storm-1.0.2/data&quot; storm文件存放目录，需要提前建立好，并赋予相关权限 supervisor.slots.ports: 相关slots端口号 - 6700 - 6701 - 6702 - 6703 ui.port: 8081 ui界面的端口号 注意每一个配置项开头必须留一个空格 ###Storm 运行 检测nimbus节点是否能正常运行 拷贝相关文件，启动整个集群 ####检测nimbus节点是否能正常运行 1.进入storm的bin目录。 2.运行./storm nimbus,如果能正常输出相关命令，说明相关软件环境已经配置完成。 3.ctr + c 杀掉刚刚启动的进程 ####拷贝相关文件，启动整个集群 1.拷贝storm软件的文件夹到集群中的每一台机器。 2.逐个启动相关软件： nimbus ： nohup ./storm nimbus &amp; supervisor : nohup ./storm supervisor &amp; 3.检查集群启动状态： 运行nohup ./storm ui &amp; 访问网址：master:8081查看相关集群的运行状态。]]></content>
      <tags>
        <tag>
- storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PCIE总线学习]]></title>
    <url>%2F2016%2F09%2F22%2Fpciezong-xian-xue-xi%2F</url>
    <content type="text"><![CDATA[PCIE总线的处理系统 PCI总线中有三类设备： PCI主设备、PCI从设备、桥设备PCI主设备—&gt;PCI Agent 网卡、显卡、声卡PCI桥可以连接两条PCI总线，上游和下游，这两个PCI总线属于同一个总线域 PCI总线的负载： PCI总线的负载与总线的频率相关，其总线频率越高，能挂的负载越少。 PCIE总线频率、带宽与负载均衡之间的关系 PCI总线传送方式：Posted和Non-Posted方式 PCI设备读写主存储器 ：DMA PCI设备读写主存储器 ：DMA 存储器预与PCI总线域的划分]]></content>
      <tags>
        <tag>
- Linux操作运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IOPS基础学习]]></title>
    <url>%2F2016%2F09%2F22%2Fiopsji-chu-xue-xi%2F</url>
    <content type="text"><![CDATA[机械硬盘的连续读写性能很好，但是随机读写的性能很差。随机读写时磁头的移动和寻道会发费很多时间。 iops–磁盘性能指标：每秒输入输出量（读写次数）单位时间内系统处理i/o请求的数量，一般以每秒io请求次数为单位。 随机读写瓶颈的应用，如小文件存储（图片）、数据库、邮件服务、很关注读写性能，iops是关键的衡量指标。 顺序读写频繁的应用，传输大量连续的数据，如电视台视频编辑、视频点播、关注连续读写性能。数据吞吐量是关键衡量指标。 IOPS和数据吞吐量适用于不同的场合： 读取10000个1KB文件，用时10秒 Throught(吞吐量)=1MB/s ，IOPS=1000 追求IOPS 读取1个10MB文件，用时0.2秒 Throught(吞吐量)=50MB/s, IOPS=5 追求吞吐量 HDD 磁盘服务时间： 完成一次io的时间：寻道时间+旋转延迟+数据传输。三部分构成 寻道时间 Tseek将磁头移动到正确的磁道上所花费的时间。一般在3-15ms 旋转延迟Trotation将数据移动至磁头下方的时间。与转速有关。7200rpm-4.17ms 15000rpm--2ms 数据传输时间Ttransfer指传输所请求数据所需要的时间，它取决于数据传输的速率。ATA可达133MB/s,SATA--300MB/s。时间可忽略。 常见硬盘的旋转延迟时间为： 7200 rpm的磁盘平均旋转延迟大约为60*1000/7200/2 = 4.17ms 10000 rpm的磁盘平均旋转延迟大约为60*1000/10000/2 = 3ms， 15000 rpm的磁盘其平均旋转延迟约为60*1000/15000/2 = 2ms。 最大IOPS的理论计算方法IOPS = 1000 ms/ (寻道时间 + 旋转延迟)。可以忽略数据传输时间。 7200 rpm的磁盘 IOPS = 1000 / (9 + 4.17) = 76 IOPS 10000 rpm的磁盘IOPS = 1000 / (6+ 3) = 111 IOPS 15000 rpm的磁盘IOPS = 1000 / (4 + 2) = 166 IOPS 影响测试的因素实际测量中，IOPS数值会受到很多因素的影响，包括I/O负载特征(读写比例，顺序和随机，工作线程数，队列深度，数据记录大小)、系统配置、操作系统、磁盘驱动等等。因此对比测量磁盘IOPS时，必须在同样的测试基准下进行，即便如此也会产生一定的随机不确定性。 队列深度说明 NCQ、SCSI TCQ、PATA TCQ和SATA TCQ技术解析 是一种命令排序技术，一把喂给设备更多的IO请求，让电梯算法和设备有机会来安排合并以及内部并行处理，提高总体效率。 SCSI TCQ的队列深度支持256级ATA TCQ的队列深度支持32级 （需要8M以上的缓存）NCQ最高可以支持命令深度级数为32级，NCQ可以最多对32个命令指令进行排序。 大多数的软件都是属于同步I/O软件，也就是说程序的一次I/O要等到上次I/O操作的完成后才进行，这样在硬盘中同时可能仅只有一个命令，也是无法发挥这个技术的优势，这时队列深度为1。 随着Intel的超线程技术的普及和应用环境的多任务化，以及异步I/O软件的大量涌现。这项技术可以被应用到了，实际队列深度的增加代表着性能的提高。 在测试时，队列深度为1是主要指标，大多数时候都参考1就可以。实际运行时队列深度也一般不会超过4. 参考连接： http://elf8848.iteye.com/blog/1731274 http://elf8848.iteye.com/blog/1731301 fio 使用学习连接： https://www.ustack.com/blog/how-benchmark-ebs/ https://segmentfault.com/a/1190000003880571 http://elf8848.iteye.com/blog/2168876]]></content>
      <tags>
        <tag>
- Linux操作运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript知识备份]]></title>
    <url>%2F2016%2F09%2F22%2Fjavascriptzhi-shi-bei-fen%2F</url>
    <content type="text"><![CDATA[##JavaScript 获取时间 var someDate = new Date(); var dateFormated = someDate.toISOString(); alert(dateFormated); json和string之间转换 JSON.stringify(obj， null, 2)将JSON转为字符串。 JSON.parse(string)将字符串转为JSON格式； js前端函数传对象获取内容 onclick=&quot;test(this)&quot; function test(obj){ console.log(obj.innerHtml); } js 字符串常用操作： 这篇文章总结的很全 http://riny.net/2012/the-summary-of-javascript-string/]]></content>
      <tags>
        <tag>
- 前端知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim/gdb操作积累备份]]></title>
    <url>%2F2016%2F09%2F22%2Fvimcao-zuo-ji-lei-bei-fen%2F</url>
    <content type="text"><![CDATA[##vim shift + G到达文件的底部gg到达文件的头部:u用于返回上一个操作 Ctrl+f和Ctrl+b是上下翻页。Ctrl+u和Ctrl+d是上下翻半页。 n : n如果是20.按下数字后再按空格。光标会向右移动20格。0 移动到这一行的开头$ 移动到这一行的结尾 :! command 暂时离开vi到指令列模式，执行command的显示结果。 :w [filename] 将编辑的数据存储为另一个文档的数据。 :r [filename] 在编辑的数据中，读入另外一个文档的数据。即将filename这个文档内容加到游标所在行的后面 :n1,n2 w [filename] 将n1到n2行的数据写入filename 区块选择（visual block）：当我们按下v或者V（ctrl + v）时就进入visual模式其他操作：ctrl+v 区块选择，可以用长方形的方式选择资料y 将反白的地方复制起来d 将反白的地方删除 多档案编辑 vim XXX1 XXX2 其他操作： :n 编辑下一个档案 :N 编辑上一个档案 :files 列出目前这个vim开启的所有档案 多窗口编辑文件： 通过vim打开一个文件之后，可以通过相应的命令打开另外一个文件，并且会在同时显示在屏幕上面。 :sp filename 其他操作 ctrl + w + 上 ctrl + w + 下 替换命令：最常用： :%s/vivian/sky/g（等同于 :g/vivian/s//sky/g） 替换每一行中所有 vivian 为 sky :[range]s/pattern/string/[c,e,g,i]5.1 range 指的是範圍，1,7 指從第一行至第七行，1,$ 指從第一行至最後一行，也就是整篇文章，也可以 % 代表。還記得嗎？ % 是目前編輯的文章，# 是前一次編輯的文章。 pattern 就是要被替換掉的字串，可以用 regexp 來表示。 string 將 pattern 由 string 所取代。 c confirm，每次替換前會詢問。 e 不顯示 error。 g globe，不詢問，整行替換。 i ignore 不分大小寫。 http://blog.csdn.net/glorin/article/details/6317098 联想目录的全路径，可以直接: cd /XXX/ 即可联想到相关路径 不必退出vim ##gdb gdb开始命令： 12345678910编译选项：gcc XX.c -o XX -g gdb &lt;program&gt;core文件：gdb &lt;program&gt; &lt;corefile&gt;服务程序运行方法：gdb &lt;program&gt; &lt;PID&gt; 可以指定piddisass main (查看函数的反汇编代码) gdb交互命令： 123456789run：简记为 r ，其作用是运行程序，当遇到断点后，程序会在断点处停止运行，等待用户输入下一步的命令。continue （简写c ）：继续执行，到下一个断点处（或运行结束）next：（简写 n），单步跟踪程序，当遇到函数调用时，也不进入此函数体；此命令同 step 的主要区别是，step 遇到用户自定义的函数，将步进到函数中去运行，而 next 则直接调用函数，不会进入到函数体内。step （简写s）：单步调试如果有函数调用，则进入函数；与命令n不同，n是不进入调用的函数的until：当你厌倦了在一个循环体内单步跟踪时，这个命令可以运行程序直到退出循环体。until+行号： 运行至某行，不仅仅用来跳出循环finish： 运行程序，直到当前函数完成返回，并打印函数返回时的堆栈地址和返回值及参数值等信息。call 函数(参数)：调用程序中可见的函数，并传递“参数”，如：call gdb_test(55)quit：简记为 q ，退出gdb gdb设置断点： 1234567891011break n （简写b n）:在第n行处设置断点（可以带上代码路径和代码名称： b OAGUPDATE.cpp:578）b fn1 if a＞b：条件断点设置break func（break缩写为b）：在函数func()的入口处设置断点，如：break cb_buttondelete 断点号n：删除第n个断点disable 断点号n：暂停第n个断点enable 断点号n：开启第n个断点clear 行号n：清除第n行的断点info b （info breakpoints） ：显示当前程序的断点设置情况delete breakpoints：清除所有断点： 查看源代码： 1234list ：简记为 l ，其作用就是列出程序的源代码，默认每次显示10行。list 行号：将显示当前文件以“行号”为中心的前后10行代码，如：list 12list 函数名：将显示“函数名”所在函数的源代码，如：list mainlist ：不带参数，将接着上一次 list 命令的，输出下边的内容。 打印表达式 1234567891011print 表达式：简记为 p ，其中“表达式”可以是任何当前正在被测试程序的有效表达式，比如当前正在调试C语言的程序，那么“表达式”可以是任何C语言的有效表达式，包括数字，变量甚至是函数调用。print a：将显示整数 a 的值print ++a：将把 a 中的值加1,并显示出来print name：将显示字符串 name 的值print gdb_test(22)：将以整数22作为参数调用 gdb_test() 函数print gdb_test(a)：将以变量 a 作为参数调用 gdb_test() 函数display 表达式：在单步运行时将非常有用，使用display命令设置一个表达式后，它将在每次单步进行指令后，紧接着输出被设置的表达式及值。如： display awatch 表达式：设置一个监视点，一旦被监视的“表达式”的值改变，gdb将强行终止正在被调试的程序。如： watch awhatis ：查询变量或函数info function： 查询函数扩展info locals： 显示当前堆栈页的所有变量 查询运行信息 123456where/bt ：当前运行的堆栈列表；bt backtrace 显示当前调用堆栈up/down 改变堆栈显示的深度set args 参数:指定运行时的参数show args：查看设置好的参数info program： 来查看程序的是否在运行，进程号，被暂停的原因。 分割窗口 12345678910layout：用于分割窗口，可以一边查看代码，一边测试：layout src：显示源代码窗口layout asm：显示反汇编窗口layout regs：显示源代码/反汇编和CPU寄存器窗口layout split：显示源代码和反汇编窗口Ctrl + L：刷新窗口refresh : 刷新窗口ctrl + x + a 退出layoutinfo win显示窗口的大小focus cmd/src/asm/regs/next/prev 切换当前窗口 多进程调试 12345678910follow-fork-mode detach-on-fork 说明parent on 只调试主进程（GDB默认）child on 只调试子进程parent off 同时调试两个进程，gdb跟主进程，子进程block在fork位置child off 同时调试两个进程，gdb跟子进程，主进程block在fork位置设置方法：set follow-fork-mode [parent|child] set detach-on-fork [on|off]info inferiors：查询正在调试的进程inferior &lt;infer number&gt; ： 切换调试的进程 gdb多线程调试 1234567info thread 查看当前进程的线程。thread &lt;ID&gt; 切换调试的线程为指定ID的线程。break file.c:100 thread all 在file.c文件第100行处为所有经过这里的线程设置断点。set scheduler-locking off|on|step，这个是问得最多的。在使用step或者continue命令调试当前被调试线程的时候，其他线程也是同时执行的，怎么只让被调试程序执行呢？通过这个命令就可以实现这个需求。off 不锁定任何线程，也就是所有线程都执行，这是默认值。on 只有当前被调试程序会执行。step 在单步的时候，除了next过一个函数的情况(熟悉情况的人可能知道，这其实是一个设置断点然后continue的行为)以外，只有当前线程会执行。]]></content>
      <tags>
        <tag>
- Linux操作运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 知识积累]]></title>
    <url>%2F2016%2F09%2F22%2Fpython-zhi-shi-ji-lei%2F</url>
    <content type="text"><![CDATA[##python 语法积累 Python 数的数据类型： type函数可以显示变量类型 字符串： 单引号和双引号使用完全相同 使用&apos;&apos;&apos; 或者 &quot;&quot;&quot;可以指定一个多行的字符串。 转移符 &apos;\&apos; 自然字符串， 通过在字符串前加r或R。 如 r&quot;this is a line with \n&quot; 则\n会显示，并不是换行。 python允许处理unicode字符串，加前缀u或U， 如 u&quot;this is an unicode string&quot;。 字符串是不可变的。 按字面意义级联字符串，如&quot;this &quot; &quot;is &quot; &quot;string&quot;会被自动转换为this is string。 对象： Python程序中用到的任何东西都将成为对象 逻辑行和物理行： 物理行是我们在编写程序时看到的，逻辑行则是python看到的。 python 中的分号；标识一个逻辑行的结束，但是实际中一般每个物理行只写一个逻辑行，可以避免使用分号。 s = &quot;peter is \ writing this article&quot; 上面\的使用被称为‘明确的行连接’ 缩进： 空白在python是非常重要的，行首的空白是最重要的，又称为缩进。行首的空白（空格和制表符）用来决定逻辑行的缩进层次，从而决定语句分组：这意味着同一层次的语句必须有相同的缩进，每一组这样的语句称为一个块。注意：不要混合使用空格和制表符来缩进，因为在跨越不同的平台时无法正常工作。 python ：控制台输出 print “abc” print “abc%s” % “d” #abcd 控制流： i = 10n = int(raw_input(“enter a number:”)) if n == i: print “equal”elif n &lt; i: print “lower”else: print “higher” while 语句： while True: passelse: pass for语句： for i in range(0,5): print ielse: pass 注：当for循环结束后执行else语句； range(a, b)返回一个序列，从a开始到b为止，但不包括b，range默认步长为1，可以指定步长，range(0,10,2)； 4. break语句 终止循环语句，如果从for或while中终止，任何对应循环的else将不执行。 5. continue语句 continue语句用来调过当前循环的剩余语句，然后继续下一轮循环。 函数： 函数通过def定义。def关键字后面跟函数的标识符名称，然后跟一对圆括号，括号里面包含一些变量名，最后以冒号结尾 def sumOf(a, b): return a + b 局部变量：在函数内定义的变量与函数外具有相同名称的其他变量没有任何关系，即变量名称对于函数来说是局部的。这称为变量的作用域。 global语句， 为定义在函数外的变量赋值时使用global语句。 def func(): global x print “x is “, x x = 1 x = 3func()print x 默认参数 通过使用默认参数可以使函数的一些参数是‘可选的’。 def say(msg, times = 1): print msg * times say(“peter”)say(“peter”, 3) 4.关键参数： def func(a, b=2, c=3): print “a is %s, b is %s, c is %s” % (a, b, c) func(1) #a is 1, b is 2, c is 3func(1, 5) #a is 1, b is 5, c is 3func(1, c = 10) #a is 1, b is 2, c is 10func(c = 20, a = 30) #a is 30, b is 2, c is 20 5.模块 模块就是一个包含了所有你定义的函数和变量的文件，模块必须以.py为扩展名。模块可以从其他程序中‘输入’(import)以便利用它的功能。 在python程序中导入其他模块使用’import’, 所导入的模块必须在sys.path所列的目录中，因为sys.path第一个字符串是空串’’即当前目录，所以程序中可导入当前目录的模块。 1. 字节编译的.pyc文件 导入模块比较费时，python做了优化，以便导入模块更快些。一种方法是创建字节编译的文件，这些文件以.pyc为扩展名。 pyc是一种二进制文件，是py文件经编译后产生的一种byte code，而且是跨平台的（平台无关）字节码，是有python虚拟机执行的，类似于 java或.net虚拟机的概念。pyc的内容，是跟python的版本相关的，不同版本编译后的pyc文件是不同的。 2. form ....import ... 如果想直接使用其他模块的变量或其他，而不加&apos;模块名+.&apos;前缀，可以使用from .. import。 例如想直接使用sys的argv，from sys import argv 或 from sys import * 六.数据结构 python有三种内建的数据结构：列表、元组和字典。 1.列表 list是处理一组有序项目的数据结构，列表是可变的数据结构。列表的项目包含在方括号[]中， eg: [1, 2, 3]， 空列表[]。判断列表中是否包含某项可以使用in， 比如 l = [1, 2, 3]; print 1 in l; #True；支持索引和切片操作；索引时若超出范围，则IndexError； 使用函数len()查看长度；使用del可以删除列表中的项，eg: del l[0] # 如果超出范围， 则IndexError append(value) count(value) extend(list2) ————————————————————————————————————————————list [] 和tuple ()的区别dict {} 和set 输入必须是一个list ([]) 重复的元素自动被过滤 的区别 set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作： http://python.usyiyi.cn/ http://www.pythondoc.com/ repr()显示函数 把变量等显示为字符串 json.dumps –&gt;把变量等转换为json格式json.loads –&gt;json转变量等 python 包结构和导入相关 http://fengwy.blog.chinaunix.net/uid-26602509-id-3499026.html 常用函数： 面向对象编程：http://www.cnblogs.com/dolphin0520/archive/2013/03/29/2986924.html 去掉字符串的换行符： for line in file.readlines(): line=line.strip(&apos;\n&apos;) python 修饰符@ http://www.cnblogs.com/xupeizhi/archive/2013/02/07/2908600.html python virtualenv 环境：http://www.cnblogs.com/kevin922/p/4315780.html 数字与字符串相互转换： string模块里有a=”12345”import stringstring.atoi(a)12345b=”123.678”string.atof(b)123.678 字符串str转换成int: int_value = int(str_value)int转换成字符串str: str_value = str(int_value)int -&gt; unicode: unicode(int_value)unicode -&gt; int: int(unicode_value)str -&gt; unicode: unicode(str_value)unicode -&gt; str: str(unicode_value)int -&gt; str: str(int_value)str -&gt; int: int(str_value)]]></content>
      <tags>
        <tag>
- python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 2.1.0安装]]></title>
    <url>%2F2016%2F09%2F19%2Fhive2-1-0%2F</url>
    <content type="text"><![CDATA[##HIVE环境搭建第一次搭建HIVE环境也是坑，由于之前没有过mysql的相关运维和开发经验，导致浪费了不少时间。 ###搭建HadoopHadoop环境一定是要的，不管你是伪分布式模式还是全分布式模式，还是HA搭建，这个必要要有。 ###HIVE安装 下载HIVE软件包 Mysql安装和配置 HIVE相关配置 ####下载HIVE软件包 我下载的2.1.0版本的：下载地址 由于2.1.0比较新，并且2.X的安装方法和1.X略有不同，相关资料也比较少，初次安装比较麻烦。 1.将 apache-hive-2.1.0-bin.tar.gz拷贝到/home/hadoop/Hive2.解压tar -zxvf apache-hive-2.1.0-bin.tar.gz3.cd /home/hadoop/Hive/apache-hive-2.1.0-bin/conf4.mv hive-default.xml.template hive-site.xml5.mv hive-env.sh.template hive-env.shHive的主要配置文件就是hive-site.xml 和 hive-env.sh ####安装mysql 安装MySQL （5.5.52） 12sudo apt-get install mysql-serversudo apt-get install mysql-client 启动MySQL/etc/init.d/mysql restart 登陆mysql做一些配置 1234mysql -uroot -pGrant all on *.* to root@’%’ identified by ‘111111’ 设置登陆相关的权限，准许相关的用户远程登陆create database hive;此库为Hive所使用的库\q； 退出 配置MySQL的配置文件 12345修改/etc/mysql/my.cnf注释掉 bind-address = 127.0.0.1如果是其他版本，把禁止远程访问的配置项去掉就行了重启MySQL 如果Hive 启动报出相关mysql链接问题，可以现在另外一台机器上面验证其是否可以远程链接MySQL。 ####修改Hive配置，启动Hive1.修改hive-env.sh12HADOOP_HOME=/usr/local/hadoopexport HIVE_CONF_DIR=/home/hadoop/Hive/apache-hive-2.1.0-bin/conf 2.修改hive.``` &lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&amp;lt;username&amp;gt; is created, with ${hive.scratch.dir.permission}.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive/local&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/tmp/hive/resources&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://master/hive&lt;/value&gt; &lt;description&gt; JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;111111&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; 此配置主要根据MySQL的数据库相关配置来设置。 3.Hadoop生成/tmp/hive目录，并改变相关权限。 hadoop fs -mkdir /tmp/hive hadoop fs -chmod 777 /tmp/hive 4.启动Hive 1. 2.x以上的Hive必须先运行：schematool -dbType mysql -initSchema 2. hive 这样就部署好了Hive… ####其他如果是网络链接不了，试一试关闭防火墙： iptables -F]]></content>
      <tags>
        <tag>
- Hadoop
- Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM基础]]></title>
    <url>%2F2016%2F09%2F19%2Fkvmji-chu%2F</url>
    <content type="text"><![CDATA[##KVM基础 ###KVM虚拟机安装 yum install qemu-kvm yum install qemu-img.x86_64 libvirt libvirt-devel libvirt-client.x86_64 安装以上软件需要OS支持 qemu-kvm是虚拟化KVM套件 libvirt是全套的虚拟化软件，并且可以使用virsh管理虚拟机 定义一个虚拟机： virsh define appserver.xml 启动一个虚拟机： virsh start appserver 进入console: virsh console appserver 直接通过命令启动虚拟机： 先生成 qemu-img create -f qcow2 test02.img 7G virt-install --name=oeltest02 --os-variant=RHEL5.8 --ram 512 --vcpus=1 --disk path=/data/test02.img,format=qcow2,size=7,bus=virtio --accelerate --cdrom /data/iso/oel58x64.iso --vnc --vncport=5910 --vnclisten=0.0.0.0 --network bridge=br0,model=virtio --noautoconsole virt-install --name=centos1 --ram 512 --vcpus=1 --disk path=/etc/libvirt/centos1.qcow2,format=qcow2,size=20,bus=virtio --cdrom /iso/CentOS-6.4-i386-LiveCD.iso --vnc --vncport=5910 --vnclisten=0.0.0.0 --network bridge=br0,model=virtio --noautoconsole SR-IOVhttp://www.cnblogs.com/sammyliu/p/4548194.html]]></content>
      <tags>
        <tag>
- 云计算
- Linux操作运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux性能监控]]></title>
    <url>%2F2016%2F09%2F19%2Flinuxxing-neng-jian-kong%2F</url>
    <content type="text"><![CDATA[##Linux常用的性能监控 基本的性能监控工具 基本的调试工具 ###基本的性能监控工具 ifstat 网络监控。 dstat： 查看系统初步的负载情况。 lsof: 查看打开的文件。 sar : sar 1 3000 用于看cpu整体的利用率 io的延迟等等 mpstat 可以查看到每一个cpu内核的运行状态。排查网卡绑定的情况可以使用。 mpstat -P ALL 1 3000 http://www.ibm.com/developerworks/cn/aix/library/au-lsof.html http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/iostat.html ###基本的调试工具 进程调试命令:truss、strace和ltrace 查看系统函数调用的热度图： yum install perf perf top ###kingson讲课学习 CPI ： 一个instruction 需要多少个cpu来跑 ， 这个越小越好 最好是0.25一个clock最好的是跑4个instruction 很好的是0.5 一般是1 1. increase CLK（and COST） increase CPU frequency increase cores increase sockets Hyper-Threading 2.Reduce CPI 3.Reduce path length SW(compiler,runtime config, programming) Better compilers Profile guided optimizations First thing to do? branch mispredict ratio cpu猜测错误的频率0.01 好 0.05差]]></content>
  </entry>
  <entry>
    <title><![CDATA[磁盘操作和iostat分析]]></title>
    <url>%2F2016%2F09%2F19%2Fci-pan-cao-zuo-he-iostatfen-xi%2F</url>
    <content type="text"><![CDATA[##磁盘运维 lsblk： 显示服务器上面块设备的相关信息 fdisk： 对容量偏小的磁盘进行相关操作 parted： 对大容量的磁盘进行相关操作 eg： parted /dev/dfc mklabel gpt mkpart 1 ext4 1 6400G http://zhangmingqian.blog.51cto.com/1497276/1068779 smartctl: 查看磁盘相关硬件信息 hdparm: hdparm 和dd差不多，是用来压测磁盘的。可以用来查看硬盘信息。 lsscsi: 查看每个硬盘的型号和厂家 df -T -h 可以查看文件系统类型 partprobe要内核更新分区表（对于磁盘分区之后，调用该命令更新内核分区表） tune2fs: 查看分区的相关信息 tune2fs -l /dev/sda5 格式化磁盘： #!/bin/bash - for i in {a..n}; do nohup &amp;&gt;/dev/null dd if=/dev/zero of=/dev/sd$i bs=1G count=4000 &amp; done ##性能监控 iotop 主要用于查看相关进程的磁盘访问情况。 iostat iostat -xk 1 无大量压力下，系统的io情况。 |字段|含义| |-------|:------------------- | |rrqm/s |(read request merge)/(读合并数 HDD)主要是合并IO，让落到磁盘上面的请求量更少| |wrqm/s |(write request merge)/(写合并数 HDD)| |r/s |read iops| |w/s |write iops| |rkB/s |read带宽| |wkB/s |write带宽| |avgrq-sz|平均每次设备I/O操作的数据大小 (扇区)| |avgqu-sz|平均I/O队列长度.即 delta(aveq)/s/1000 (因为aveq的单位为毫秒)| |await |平均每次设备I/O操作的等待时间 (毫秒). | |r_await | 平均每次设备I/O读操作的等待时间 (毫秒).| |w_await | 平均每次设备I/O写操作的等待时间 (毫秒).| |svctm | 平均每次设备I/O操作的服务时间 (毫秒).即 delta(use)/delta(rio+wio)| |util |一秒中有百分之多少的时间用于 I/O 操作,或者说一秒中有多少时间 I/O 队列是非空的.即 delta(use)/s/1000 (因为use的单位为毫秒)| 一般ssd的await 在190左右 http://www.ha97.com/4546.htmlhttp://www.php-oa.com/2009/02/03/iostat.html ##磁盘和Linux驱动关系目前很多磁盘为了增大空间，会有一个逻辑扇区和物理扇区。 物理扇区是磁盘物理存储的最小单元，逻辑扇区是为了兼容Linux内核扇区的大小，现在Linux内核默认的扇区大小为512。逻辑扇区是OS访问磁盘的最小单元。iostat出现的avgrq-sz的单位就是逻辑扇区。 物理扇区和逻辑扇区会产生对齐的问题，在格式化磁盘的时候要注意。 http://noops.me/?p=747 ##Inode和文件存储 文件存储在硬盘上，硬盘的最小操作单元是扇区，每个扇区是512字节. 操作系统操作磁盘的时候不会直接去操作扇区，这样效率很低。操作系统一般是操作block，每个block是4K. 分区上的文件系统相关信息可以通过：tune2fs -l /dev/sda1 或者 dumpe2fs -h /dev/sda1来查看。 文件的数据都存储在”块”中，相关的文件原信息存储在inode中。 inode一般存储的是文件的名称、修改时间、权限、创建者、文件大小等等。 文件的inode信息可以通过stat XXX查看到。 inode的大小会消耗磁盘空间，所以磁盘格式化的时候，操作系统会自动将硬盘分为两个区域，一个是数据区，存放真实的数据；一个是inode区，存放inode 相关的信息。（df -i） io细致图整体的逻辑架构图IO经典架构图 http://way4ever.com/?p=359]]></content>
      <tags>
        <tag>
- Linux操作运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS原理和简单使用]]></title>
    <url>%2F2016%2F09%2F18%2Fhdfsgao-ke-yong-bu-shu-fang-an%2F</url>
    <content type="text"><![CDATA[##HDFS原理和简单使用 namenode datanode 读写流程 ###namenode Namenode 管理文件系统的命名空间。他维护着文件系统树及整棵树内的所有文件和目录。这些信息以两个文件的形式永久保存在本地的磁盘上：命名空间镜像文件和编辑日志文件。Namenode 也记录着每个文件中各个块所在的数据节点信息，但他并不是永久保存块的位置信息，因为这些信息会在系统启动的时候由数据节点重建。 高可用方案：namenode存在单点失效的问题。在实现中，配置了一对活动-备用的namenode。当活动namenode失效，备用的namenode就会接管它的工作任务并开始服务于来自客服端的请求，不会有任何明显的中断。 namenode之间需要通过高可用的共享存储实现编辑日志的共享。 datanode需要同时向两个namenode发送数据块处理报告，因为数据块的映射信息存储在namenode的内存中，而非磁盘。 客服端需要使用特定的机制来处理namenode的失效问题，这一机制对用户是透明的。 扩展方案：联邦HDFS。在联邦环境下，每一个namenode维护一个名称空间卷，包括命名空间的元数据和在该命名空间下的文件的所有数据块和数据块池。 ###datanodeDatanode 是文件系统的工作节点。它们需要存储并检索数据块，并且定期向namenode发送它们所存储的块的列表。 ###读写流程 HDFS 读数据流程一般先会创建文件系统的类，通过这个类去访问NameNode上面的相关数据。 1. 通过DistributedFileSystem类从NameNode 上获取所需文件对应的块位置。 2. 然后通过FSDataInputStream类读取块所在DataNode上面具体块的数据。 3. 数据返回后进行相应的拼接，形成具体的文件。 HDFS写流程和读流程类似，先通过文件系统类去namenode上获取创建文件的块信息。然后根据块信息去写入文件。 1. 通过DistributedFileSystem类从NameNode 上获取创建文件对应的块信息。 2. 然后通过FSDataInputStream类将信息写入某一个datanode中，第一个datanode会依此向后面的datanode写入相关的数据，并且最后一个datanode会依此将写入成功的信息返回给前面的datanode。 3. 最后一个块写入完成后，返回相应的写入完成信息，关闭相关的流和资源。 ###HDFS数据的完整性和压缩完整性： 1.HDFS会对写入的所有数据计算校验和，并在读取数据时验证校验和。它针对每个由io.bytes.per.checksum指定字节的数据计算校验和。默认情况下为512。由于CRC-32校验和是4个字节，所以存储的校验和的额外资源低于1%。 2.每一个datanode也会在一个后台线程中运行一个DataBlockScanner，从而定期的验证存储在这个datanode上的所有数据块。 3.主要操作类LocalFileSystem 和ChecksumFileSystem 压缩： 1. 减少存储文件所需要的磁盘空间，加速数据在网络和磁盘上的传输。 序列化 序列化是指将结构化对象转化为字节流以便在网络上传输或写到磁盘进行永久存储的过程。反序列化是指将字节流转回结构化对象的逆过程。 出现领域：进程间的通信和永久存储 hadoop使用的是自己的序列化格式Writable。 ###HDFS与POSIX接口不同之处 新建一个文件之后，它能在文件系统的命名空间中立即可见。但是写入文件的内容并不保证能立即可见，即使数据流已经刷新并存储。当写入的数据超过一个块后，第一个数据块对新的reader就是可见的。之后的数据块也不例外。总之当前正在写入的块对其他的reader不可见。 HDFS提供一个方法来使所有的缓存与数据节点强行同步，即对FSDataOutputStream调用sync()方法。（该方法在hadoop的后期版本中已经被替换为hsync 、hflush） ###相关API的使用 123456789101112FileSystem.get(URI.create(uri), conf) 获得文件系统的操作句柄（FSDataInputStream）InputStream in = fs.open(new Path(uri)) 获得文件句柄IOUtils.copyBytes(in, System.out, 4096, false) 拷贝相关的数据In.seek() 移动文件的位置fs.create()创建文件FSDataInputStream 操作输入流FSDataOutputStream 操作输出流Fs.mkdirs创建文件夹Fs.delete()删除数据文件夹操作：FileStatus: 封装了文件系统中文件和目录的元数据，包括文件的长度、块大小、副本、修改时间、所有者以及权限信息。]]></content>
      <tags>
        <tag>
- Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux启动过程]]></title>
    <url>%2F2016%2F09%2F18%2Flinuxqi-dong-guo-cheng%2F</url>
    <content type="text"><![CDATA[##Linux 启动过程 vmlinuxz是真正的内核文件，initramfs用来协助内核挂载文件系统。/usr/src/kernels下面存放的是内核的源码和相关编译好的ko文件。 Linux 启动过程图 pxe启动过程： PXE Client 向 UDP 67端口 广播 DHCPDDISCOVER 消息. DHCP SERVER 或者 DHCP Proxy 收到广播消息后,发送DHCPOFFER(包含ip地址)消息 到 PXE Client的 68 端口. PXE Client 发送 DHCPREQUEST 消息到 DHCP SERVER ,获取启动文件(boot file name). DHCP SERVER 发送DHCPACK(包含Network Bootstrap Program file name)消息 到PXE Client. PXE Client 向 Boot Server 获取 NBP(Network Bootstrap Program) 文件. PXE Client 从 TFTP SERVER 下载 NBP,然后在客户端执行NBP文件 此时已经获取到NBP文件： pxelinux.0(NBP)文件：获取安装文件 RAMOS 忘记root密码： grub页面进入 single 修改完密码之后init 5 系统出问题进入不了OS grub界面修改启动程序 init=/bin/bash 此时系统只是挂载了跟目录，并且为只读。 mount -o remount,rw / 相关细节： http://www.cnblogs.com/wwang/archive/2010/10/27/1862222.htmlhttp://haobo.blog.51cto.com/2893071/589479 http://blog.csdn.net/jx_jy/article/details/12783559 http://www.doc88.com/p-474114520477.html]]></content>
      <tags>
        <tag>
- Linux操作运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础操作总结]]></title>
    <url>%2F2016%2F09%2F17%2Flinuxji-chu-cao-zuo-zong-jie%2F</url>
    <content type="text"><![CDATA[##Linux基础操作 ###samba服务器搭建 yum install samba yum install samba-common yum install samba-client smbpasswd -a shaolong.psl service smb restart 安装相关软件，设置相关用户的密码 ###VirtualBox设置共享文件夹虚拟机为ubuntu，物理机为window。 设置共享文件夹： &quot;mount -t vboxsf BaiduShare /mnt/bdshare/&quot; ###ipmitool相关操作 开启ipmi服务： service ipmi start 查看BMC lan口信息 ipmitool lan print 通过ipmitool命令点灯。此方法用于在机房定位问题服务器。 ipmitool chassis identify 服务器进入PXE启动方式 ipmitool chassis bootdev pxe ipmitool raw 0 2 3 改变服务器引导方式 ipmitool -I lan -H 10.1.199.212 -U ADMIN -P ADMIN chassis bootdev pxe ipmitool -I lan -H 10.1.199.212 -U ADMIN -P ADMIN chassis bootdev disk ipmitool -I lan -H 10.1.199.212 -U ADMIN -P ADMIN chassis bootdev cdrom 服务器电源管理 ipmitool -I lan -H 10.1.199.212 -U ADMIN -P ADMIN chassis power off ipmitool -I lan -H 10.1.199.212 -U ADMIN -P ADMIN chassis power reset mitool -I lan -H 10.1.199.212 -U ADMIN -P ADMIN chassis power on ipmitool -I lan -H 10.1.199.212 -U ADMIN -P ADMIN chassis power status 登录oob1.tbc ipmitool -I lanplus -H 10.1.199.XXX -U ADMIN -P ADMIN power reset ipmitool -I lanplus -H 10.1.199.XXX -U ADMIN -P ADMIN sol activate ###Linux相关运维必备命令 查看服务器运行时间和相关平均负载 uptime 查看板载硬件相关信息 dmidecode 进入无盘后修改机器密码 mount /dev/sda2 /mnt 挂载OS盘到临时目录 cd /mnt 进入临时目录 chroot . 将当前目录修改为根目录 passwd 修改当前os密码 查看Linux开启的端口 netstat -nplt 查看完整进程命令 ps -efwww 进程状态控制 ctrl + z 挂起前端的进程 此时进程被挂起 得不到cpu的执行 fg +number 将挂起的进程恢复到前台 jobs显示被挂起的进程 bg + number 将挂起的进程 启动得到cpu的执行 window和linux传输文件方式 可以安装一个lrzsz sudo apt-get install lrzsz 通过sz和rz传输文件 磁盘做LV： aliflash 创建pv需要修改/etc/lvm/lvm.conf types ===&gt; cat /proc/devise 创建pv pvcreate 创建vg vgcreate 后台运行程序多种方法 https://www.ibm.com/developerworks/cn/linux/l-cn-nohup/ nohup cmd &amp; screen -S XXX 查看Linux所安装的软件包 rpm -qa 修改root密码 sudo passwd 添加用户 http://blog.51yip.com/linux/1137.html useradd test passwd test usermod -d /home/test -G test2 test userdel test last查看登录成功的用户记录 lastb查看登录失败的用户记录 查看usb相关设备 lsusb lsof： http://man.linuxde.net/lsof http://blog.csdn.net/wyzxg/article/details/4971843 http://blog.csdn.net/kozazyh/article/details/5495532 lspci查看pcie设备 tr替换： http://man.linuxde.net/tr echo &quot;HELLO WORLD&quot; | tr &apos;A-Z&apos; &apos;a-z&apos; du显示文件大小 du -h du -ah du -h --max-depth=1 #-h:用K、M、G的人性化形式显示 #-a:显示目录和文件 查看文件夹大小： du -sh XXX 查看占用CPU最大的进程 top 1 i c 可以看到占用cpu最大的进程 shell命令运行符号&amp;;&amp;&amp; command1&amp;command2&amp;command3 三个命令同时执行 command1;command2;command3 不管前面命令执行成功没有，后面的命令继续执行 command1&amp;&amp;command2 只有前面命令执行成功，后面命令才继续执行 ###查看Linux版本信息 cat /etc/issue uname -a cat /proc/version lsb_release -a cat /etc/redhat-release 应对不同的Linux发行版本，查看系统版本信息各不同，目前主要使用cat /etc/redhat-release ###常用Linux命令 ####xargs http://man.linuxde.net/xargs xargs命令是给其他命令传递参数的一个过滤器，也是组合多个命令的一个工具。它擅长将标准输入数据转 换成命令行参数，xargs能够处理管道或者stdin并将其转换成特定命令的命令参数。xargs也可以将单行 或多行文本输入转换为其他格式，例如多行变单行，单行变多行。xargs的默认命令是echo，空格是默认定 界符。这意味着通过管道传递给xargs的输入将会包含换行和空白，不过通过xargs的处理，换行和空白将 被空格取代。xargs是构建单行命令的重要组件之一. 常用命令： ps -ef | grep XXX | awk &apos;{print $2}&apos; | xargs kill -9 ####grep http://man.linuxde.net/grep grep（global search regular expression(RE) and print out the line，全面搜索正则表达式 并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 在多级目录中对文本进行递归搜索： grep “text” . -r -n grep -rn “add_task_manual_submit” * ####find http://man.linuxde.net/find find命令用来在指定目录下查找文件。任何位于参数之前的字符串都将被视为欲查找的目录名。如果使用该命令时，不设置任何参数，则find命令将在当前目录下查找子目录与文件。并且将查找到的子目录和文件全部进行显示。 find /home -name “*.txt” ####pgm 安装： yum install tops-pgm 不行的话最后面加* yum install taobao-pgm pgm主要是用于批量分发命令。对批量操作机器很有帮助。 拷贝文件： pgmscp -A -f ip.list -l root test.sh /home/ 批处理文件： pgm -b -p 10 -f ip.list -l root -A &apos;pwd&apos; ####ps ps 命令： ps工具标识进程的5种状态码: D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process a 显示所有进程 -a 显示同一终端下的所有程序 -A 显示所有进程 c 显示进程的真实名称 -N 反向选择 -e 等于“-A” e 显示环境变量 f 显示程序间的关系 -H 显示树状结构 r 显示当前终端的进程 T 显示当前终端的所有程序 u 指定用户的所有进程 -au 显示较详细的资讯 -aux 显示所有包含其他使用者的行程 -C&lt;命令&gt; 列出指定命令的状况 --lines&lt;行数&gt; 每页显示的行数 --width&lt;字符数&gt; 每页显示的字符数 --help 显示帮助信息 --version 显示版本显示 ps -A ps -u root ps -ef ps -l 将目前属于您自己这次登入的 PID 与相关信息列示出来 ps aux 所有的正在内存中的程序 ###ulimit 用来设置系统的资源限度。比如用来调节进程拥有的文件描述符数量 http://man.linuxde.net/ulimit https://www.ibm.com/developerworks/cn/linux/l-cn-ulimit/ ###防火墙 SElinux操作： /usr/sbin/sestatus -v setenforce 0 #临时关闭SElinux,临时打开SElinux状态命令：setenforce 1 vim /etc/selinux/config,然后将SELINUX改成disabled，这样即使重启操作系统SElinux也是关闭状态。 iptables： service iptables status getenforce #service iptables stop #临时关闭防火墙 #chkconfig iptables off #永久关闭防火墙，开机不自启动，想自启动改成on ###服务器硬件基础知识 cpu互联通过QPI总线 能保证cache的一致性 cpu与PCH互联DMI总线 CPU和高速设备连接一般是pcie SATA和sas协议 cpu和内存连接是通过ddr4 ###系统日志 Linux日志输出文件和相关含义 Linux日志级别]]></content>
      <tags>
        <tag>
- Linux操作运维</tag>
      </tags>
  </entry>
</search>
